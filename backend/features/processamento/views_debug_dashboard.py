# backend/features/processamento/views_debug_dashboard.py

from rest_framework import status
from rest_framework.decorators import api_view, permission_classes
from rest_framework.permissions import IsAuthenticated
from rest_framework.response import Response
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator
from django.views.decorators.cache import never_cache
from django.http import JsonResponse
from django.core.cache import cache
from django.db.models import Q, Avg, Count, Sum, F, Max, Min
from datetime import datetime, timedelta
from django.utils import timezone
import json
import logging

from .models import ShopifyConfig, IPDetectionStatistics, IPDetectionDebugLog, IPDetectionAlert
from .services.structured_logging_service import get_structured_logging_service
from .utils.security_utils import SecurityHeadersManager, AuditLogger

logger = logging.getLogger(__name__)

@csrf_exempt
@api_view(['GET'])
@permission_classes([IsAuthenticated])
@never_cache
def get_debug_statistics(request):
    """
    Endpoint principal para métricas de debug: /api/processamento/debug-stats/
    Retorna dashboard completo de estatísticas de detecção de IP
    """
    try:
        # Parâmetros de consulta
        loja_id = request.GET.get('loja_id')
        period = request.GET.get('period', '24h')  # 24h, 7d, 30d
        include_trends = request.GET.get('include_trends', 'true').lower() == 'true'
        include_alerts = request.GET.get('include_alerts', 'true').lower() == 'true'
        
        # Validações
        if loja_id:
            try:
                loja_id = int(loja_id)
                config = ShopifyConfig.objects.filter(id=loja_id, ativo=True).first()
                if not config:
                    return Response({
                        'error': 'Loja não encontrada ou inativa'
                    }, status=status.HTTP_404_NOT_FOUND)
            except ValueError:
                return Response({
                    'error': 'ID da loja inválido'
                }, status=status.HTTP_400_BAD_REQUEST)
        
        # Converte período para horas
        period_hours = {
            '1h': 1,
            '6h': 6, 
            '24h': 24,
            '7d': 24 * 7,
            '30d': 24 * 30
        }.get(period, 24)
        
        # Obtém serviço de logging
        logging_service = get_structured_logging_service()
        
        # Estatísticas em tempo real
        if loja_id:
            realtime_stats = logging_service.get_realtime_statistics(loja_id, period_hours)
            configs_analyzed = [config]
        else:
            # Estatísticas agregadas para todas as lojas do usuário
            user_configs = ShopifyConfig.objects.filter(ativo=True)
            realtime_stats = _aggregate_multi_store_stats(user_configs, period_hours)
            configs_analyzed = list(user_configs)
        
        dashboard_data = {
            'timestamp': timezone.now().isoformat(),
            'period_analyzed': period,
            'loja_especifica': bool(loja_id),
            'config_info': {
                'id': config.id,
                'nome_loja': config.nome_loja,
                'shop_url': config.shop_url
            } if loja_id else {
                'total_lojas': len(configs_analyzed),
                'lojas': [{'id': c.id, 'nome': c.nome_loja} for c in configs_analyzed]
            },
            'realtime_statistics': realtime_stats
        }
        
        # Análise de tendências (se solicitado)
        if include_trends:
            if loja_id:
                trend_analysis = logging_service.get_trend_analysis(loja_id, 30)
                dashboard_data['trend_analysis'] = trend_analysis
            else:
                dashboard_data['trend_analysis'] = {
                    'message': 'Análise de tendência disponível apenas para loja específica',
                    'suggestion': 'Adicione parâmetro loja_id para análise detalhada'
                }
        
        # Alertas ativos (se solicitado)
        if include_alerts:
            if loja_id:
                active_alerts = list(IPDetectionAlert.objects.filter(
                    config_id=loja_id,
                    status='active'
                ).order_by('-severidade', '-primeira_ocorrencia')[:10].values(
                    'id', 'tipo_alerta', 'severidade', 'titulo', 'descricao',
                    'primeira_ocorrencia', 'contagem_ocorrencias'
                ))
                
                # Verifica por novos alertas
                new_alerts = logging_service.check_for_alerts(loja_id)
                
                dashboard_data['alerts'] = {
                    'active_alerts': active_alerts,
                    'total_active': len(active_alerts),
                    'new_alerts_detected': len(new_alerts),
                    'new_alerts': new_alerts[:5]  # Primeiros 5
                }
            else:
                # Alertas agregados para todas as lojas
                all_alerts = []
                for config in configs_analyzed:
                    config_alerts = list(IPDetectionAlert.objects.filter(
                        config=config,
                        status='active'
                    ).values(
                        'id', 'tipo_alerta', 'severidade', 'titulo',
                        'primeira_ocorrencia', 'config__nome_loja'
                    )[:5])
                    all_alerts.extend(config_alerts)
                
                dashboard_data['alerts'] = {
                    'aggregated_alerts': sorted(all_alerts, 
                                              key=lambda x: x['primeira_ocorrencia'], 
                                              reverse=True)[:20],
                    'total_active_all_stores': len(all_alerts)
                }
        
        # Diagnóstico automático
        diagnostic = _run_automated_diagnostic(configs_analyzed, period_hours)
        dashboard_data['automated_diagnostic'] = diagnostic
        
        # Recomendações automáticas
        recommendations = _generate_automatic_recommendations(realtime_stats, dashboard_data)
        dashboard_data['recommendations'] = recommendations
        
        # Auditoria de acesso
        AuditLogger.log_ip_access(
            request.user,
            request,
            'debug_dashboard_access',
            {
                'loja_id': loja_id,
                'period': period,
                'include_trends': include_trends,
                'include_alerts': include_alerts,
                'configs_count': len(configs_analyzed)
            }
        )
        
        response = Response({
            'success': True,
            'dashboard_data': dashboard_data
        })
        
        return SecurityHeadersManager.add_security_headers(response)
        
    except Exception as e:
        logger.error(f"Erro no dashboard de debug: {e}")
        return Response({
            'error': 'Erro interno do servidor',
            'details': str(e) if request.user.is_staff else 'Entre em contato com o suporte'
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n\n@csrf_exempt\n@api_view(['GET'])\n@permission_classes([IsAuthenticated])\n@never_cache\ndef get_performance_metrics(request):\n    \"\"\"\n    Métricas detalhadas de performance com histórico\n    /api/processamento/performance-metrics/\n    \"\"\"\n    try:\n        loja_id = request.GET.get('loja_id')\n        hours = int(request.GET.get('hours', 24))\n        \n        if not loja_id:\n            return Response({\n                'error': 'ID da loja é obrigatório'\n            }, status=status.HTTP_400_BAD_REQUEST)\n        \n        config = ShopifyConfig.objects.filter(id=loja_id, ativo=True).first()\n        if not config:\n            return Response({\n                'error': 'Loja não encontrada'\n            }, status=status.HTTP_404_NOT_FOUND)\n        \n        since = timezone.now() - timedelta(hours=hours)\n        \n        # Consultas de performance\n        debug_logs = IPDetectionDebugLog.objects.filter(\n            config=config,\n            timestamp__gte=since,\n            tempo_processamento_ms__isnull=False\n        )\n        \n        # Métricas básicas\n        performance_stats = debug_logs.aggregate(\n            total_detections=Count('id'),\n            avg_time=Avg('tempo_processamento_ms'),\n            min_time=Min('tempo_processamento_ms'),\n            max_time=Max('tempo_processamento_ms'),\n            total_time=Sum('tempo_processamento_ms')\n        )\n        \n        # Distribuição de tempos\n        time_distribution = {\n            'very_fast': debug_logs.filter(tempo_processamento_ms__lt=1000).count(),  # < 1s\n            'fast': debug_logs.filter(\n                tempo_processamento_ms__gte=1000,\n                tempo_processamento_ms__lt=3000\n            ).count(),  # 1-3s\n            'moderate': debug_logs.filter(\n                tempo_processamento_ms__gte=3000,\n                tempo_processamento_ms__lt=5000\n            ).count(),  # 3-5s\n            'slow': debug_logs.filter(\n                tempo_processamento_ms__gte=5000,\n                tempo_processamento_ms__lt=10000\n            ).count(),  # 5-10s\n            'very_slow': debug_logs.filter(tempo_processamento_ms__gte=10000).count()  # > 10s\n        }\n        \n        # Performance por método\n        method_performance = debug_logs.filter(\n            metodo_deteccao__isnull=False\n        ).values('metodo_deteccao').annotate(\n            count=Count('id'),\n            avg_time=Avg('tempo_processamento_ms'),\n            success_rate=Avg(\n                Count('id', filter=Q(ip_detectado__isnull=False)) * 100.0 / Count('id')\n            )\n        ).order_by('-count')\n        \n        # Timeline de performance (últimas 24 buckets)\n        timeline = _create_performance_timeline(debug_logs, hours)\n        \n        performance_data = {\n            'config_info': {\n                'id': config.id,\n                'nome_loja': config.nome_loja\n            },\n            'period_analyzed_hours': hours,\n            'summary_statistics': performance_stats,\n            'time_distribution': time_distribution,\n            'method_performance': list(method_performance),\n            'performance_timeline': timeline,\n            'insights': _generate_performance_insights(performance_stats, time_distribution)\n        }\n        \n        response = Response({\n            'success': True,\n            'performance_data': performance_data\n        })\n        \n        return SecurityHeadersManager.add_security_headers(response)\n        \n    except Exception as e:\n        logger.error(f\"Erro nas métricas de performance: {e}\")\n        return Response({\n            'error': 'Erro interno do servidor'\n        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n\n@csrf_exempt\n@api_view(['GET'])\n@permission_classes([IsAuthenticated])\n@never_cache\ndef get_detailed_logs(request):\n    \"\"\"\n    Logs detalhados com filtros avançados\n    /api/processamento/detailed-logs/\n    \"\"\"\n    try:\n        loja_id = request.GET.get('loja_id')\n        nivel = request.GET.get('nivel')  # DEBUG, INFO, WARNING, ERROR, CRITICAL\n        categoria = request.GET.get('categoria')\n        order_id = request.GET.get('order_id')\n        session_id = request.GET.get('session_id')\n        hours = int(request.GET.get('hours', 24))\n        page_size = min(int(request.GET.get('page_size', 50)), 100)  # Máximo 100\n        page = int(request.GET.get('page', 1))\n        \n        if not loja_id:\n            return Response({\n                'error': 'ID da loja é obrigatório'\n            }, status=status.HTTP_400_BAD_REQUEST)\n        \n        config = ShopifyConfig.objects.filter(id=loja_id, ativo=True).first()\n        if not config:\n            return Response({\n                'error': 'Loja não encontrada'\n            }, status=status.HTTP_404_NOT_FOUND)\n        \n        since = timezone.now() - timedelta(hours=hours)\n        \n        # Constrói query com filtros\n        queryset = IPDetectionDebugLog.objects.filter(\n            config=config,\n            timestamp__gte=since\n        )\n        \n        if nivel:\n            queryset = queryset.filter(nivel=nivel.upper())\n        \n        if categoria:\n            queryset = queryset.filter(categoria=categoria)\n        \n        if order_id:\n            queryset = queryset.filter(order_id=order_id)\n        \n        if session_id:\n            queryset = queryset.filter(session_id=session_id)\n        \n        # Paginação\n        total_count = queryset.count()\n        offset = (page - 1) * page_size\n        logs = queryset.order_by('-timestamp')[offset:offset + page_size]\n        \n        # Serializa logs\n        logs_data = []\n        for log in logs:\n            log_data = {\n                'id': log.id,\n                'timestamp': log.timestamp.isoformat(),\n                'nivel': log.nivel,\n                'categoria': log.categoria,\n                'subcategoria': log.subcategoria,\n                'titulo': log.titulo,\n                'order_id': log.order_id,\n                'session_id': log.session_id,\n                'ip_detectado': log.ip_detectado,\n                'metodo_deteccao': log.metodo_deteccao,\n                'score_confianca': log.score_confianca,\n                'tempo_processamento_ms': log.tempo_processamento_ms,\n                'detalhes_json': log.detalhes_json,\n                'user_agent': log.user_agent[:100] if log.user_agent else '',  # Trunca\n                'ip_requisicao': log.ip_requisicao\n            }\n            logs_data.append(log_data)\n        \n        # Estatísticas dos logs filtrados\n        filter_stats = {\n            'total_logs': total_count,\n            'level_distribution': dict(queryset.values('nivel').annotate(\n                count=Count('id')\n            ).values_list('nivel', 'count')),\n            'category_distribution': dict(queryset.values('categoria').annotate(\n                count=Count('id')\n            ).values_list('categoria', 'count'))\n        }\n        \n        response_data = {\n            'success': True,\n            'config_info': {\n                'id': config.id,\n                'nome_loja': config.nome_loja\n            },\n            'filters_applied': {\n                'nivel': nivel,\n                'categoria': categoria,\n                'order_id': order_id,\n                'session_id': session_id,\n                'hours': hours\n            },\n            'pagination': {\n                'page': page,\n                'page_size': page_size,\n                'total_count': total_count,\n                'total_pages': (total_count + page_size - 1) // page_size,\n                'has_next': offset + page_size < total_count,\n                'has_previous': page > 1\n            },\n            'filter_statistics': filter_stats,\n            'logs': logs_data\n        }\n        \n        response = Response(response_data)\n        return SecurityHeadersManager.add_security_headers(response)\n        \n    except Exception as e:\n        logger.error(f\"Erro ao obter logs detalhados: {e}\")\n        return Response({\n            'error': 'Erro interno do servidor'\n        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n\n@csrf_exempt\n@api_view(['GET', 'POST'])\n@permission_classes([IsAuthenticated])\n@never_cache\ndef manage_alerts(request):\n    \"\"\"\n    Gerenciamento de alertas: listar, reconhecer, resolver\n    /api/processamento/alerts/\n    \"\"\"\n    try:\n        if request.method == 'GET':\n            # Listar alertas\n            loja_id = request.GET.get('loja_id')\n            status_filter = request.GET.get('status', 'active')  # active, acknowledged, resolved\n            severidade = request.GET.get('severidade')  # low, medium, high, critical\n            page_size = min(int(request.GET.get('page_size', 20)), 50)\n            page = int(request.GET.get('page', 1))\n            \n            # Constrói query\n            queryset = IPDetectionAlert.objects.all()\n            \n            if loja_id:\n                config = ShopifyConfig.objects.filter(id=loja_id, ativo=True).first()\n                if not config:\n                    return Response({\n                        'error': 'Loja não encontrada'\n                    }, status=status.HTTP_404_NOT_FOUND)\n                queryset = queryset.filter(config=config)\n            \n            if status_filter:\n                queryset = queryset.filter(status=status_filter)\n            \n            if severidade:\n                queryset = queryset.filter(severidade=severidade)\n            \n            # Paginação\n            total_count = queryset.count()\n            offset = (page - 1) * page_size\n            alerts = queryset.order_by('-severidade', '-primeira_ocorrencia')[offset:offset + page_size]\n            \n            # Serializa alertas\n            alerts_data = []\n            for alert in alerts:\n                alert_data = {\n                    'id': alert.id,\n                    'config_info': {\n                        'id': alert.config.id,\n                        'nome_loja': alert.config.nome_loja\n                    },\n                    'tipo_alerta': alert.tipo_alerta,\n                    'severidade': alert.severidade,\n                    'status': alert.status,\n                    'titulo': alert.titulo,\n                    'descricao': alert.descricao,\n                    'valor_detectado': alert.valor_detectado,\n                    'threshold_configurado': alert.threshold_configurado,\n                    'periodo_referencia': alert.periodo_referencia,\n                    'sugestao_acao': alert.sugestao_acao,\n                    'primeira_ocorrencia': alert.primeira_ocorrencia.isoformat(),\n                    'ultima_ocorrencia': alert.ultima_ocorrencia.isoformat(),\n                    'contagem_ocorrencias': alert.contagem_ocorrencias,\n                    'reconhecido_por': alert.reconhecido_por.username if alert.reconhecido_por else None,\n                    'reconhecido_em': alert.reconhecido_em.isoformat() if alert.reconhecido_em else None,\n                    'pode_resolver_automaticamente': alert.pode_resolver_automaticamente\n                }\n                alerts_data.append(alert_data)\n            \n            response_data = {\n                'success': True,\n                'pagination': {\n                    'page': page,\n                    'page_size': page_size,\n                    'total_count': total_count,\n                    'total_pages': (total_count + page_size - 1) // page_size\n                },\n                'alerts': alerts_data,\n                'summary': {\n                    'by_severity': dict(queryset.values('severidade').annotate(\n                        count=Count('id')\n                    ).values_list('severidade', 'count')),\n                    'by_type': dict(queryset.values('tipo_alerta').annotate(\n                        count=Count('id')\n                    ).values_list('tipo_alerta', 'count'))\n                }\n            }\n            \n            response = Response(response_data)\n            return SecurityHeadersManager.add_security_headers(response)\n            \n        elif request.method == 'POST':\n            # Ações em alertas (acknowledge, resolve)\n            alert_id = request.data.get('alert_id')\n            action = request.data.get('action')  # acknowledge, resolve, false_positive\n            \n            if not alert_id or not action:\n                return Response({\n                    'error': 'alert_id e action são obrigatórios'\n                }, status=status.HTTP_400_BAD_REQUEST)\n            \n            alert = IPDetectionAlert.objects.filter(id=alert_id).first()\n            if not alert:\n                return Response({\n                    'error': 'Alerta não encontrado'\n                }, status=status.HTTP_404_NOT_FOUND)\n            \n            # Executa ação\n            if action == 'acknowledge':\n                alert.acknowledge(request.user)\n                message = 'Alerta reconhecido com sucesso'\n            elif action == 'resolve':\n                alert.resolve()\n                message = 'Alerta resolvido com sucesso'\n            elif action == 'false_positive':\n                alert.status = 'false_positive'\n                alert.save()\n                message = 'Alerta marcado como falso positivo'\n            else:\n                return Response({\n                    'error': 'Ação inválida. Use: acknowledge, resolve, false_positive'\n                }, status=status.HTTP_400_BAD_REQUEST)\n            \n            # Log da ação\n            AuditLogger.log_ip_access(\n                request.user,\n                request,\n                'alert_action',\n                {\n                    'alert_id': alert_id,\n                    'action': action,\n                    'alert_title': alert.titulo,\n                    'config_id': alert.config.id\n                }\n            )\n            \n            return Response({\n                'success': True,\n                'message': message,\n                'alert_status': alert.status\n            })\n            \n    except Exception as e:\n        logger.error(f\"Erro no gerenciamento de alertas: {e}\")\n        return Response({\n            'error': 'Erro interno do servidor'\n        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n\n@csrf_exempt\n@api_view(['POST'])\n@permission_classes([IsAuthenticated])\n@never_cache\ndef run_diagnostic(request):\n    \"\"\"\n    Executa diagnóstico manual detalhado\n    /api/processamento/run-diagnostic/\n    \"\"\"\n    try:\n        loja_id = request.data.get('loja_id')\n        diagnostic_type = request.data.get('type', 'full')  # full, performance, data_quality\n        \n        if not loja_id:\n            return Response({\n                'error': 'ID da loja é obrigatório'\n            }, status=status.HTTP_400_BAD_REQUEST)\n        \n        config = ShopifyConfig.objects.filter(id=loja_id, ativo=True).first()\n        if not config:\n            return Response({\n                'error': 'Loja não encontrada'\n            }, status=status.HTTP_404_NOT_FOUND)\n        \n        # Executa diagnóstico baseado no tipo\n        diagnostic_results = {}\n        \n        if diagnostic_type in ['full', 'performance']:\n            # Diagnóstico de performance\n            performance_diag = _run_performance_diagnostic(config)\n            diagnostic_results['performance'] = performance_diag\n        \n        if diagnostic_type in ['full', 'data_quality']:\n            # Diagnóstico de qualidade de dados\n            data_quality_diag = _run_data_quality_diagnostic(config)\n            diagnostic_results['data_quality'] = data_quality_diag\n        \n        if diagnostic_type == 'full':\n            # Diagnóstico completo adicional\n            system_health = _run_system_health_diagnostic(config)\n            diagnostic_results['system_health'] = system_health\n            \n            # Recomendações específicas\n            recommendations = _generate_diagnostic_recommendations(diagnostic_results)\n            diagnostic_results['recommendations'] = recommendations\n        \n        # Log do diagnóstico\n        AuditLogger.log_ip_access(\n            request.user,\n            request,\n            'manual_diagnostic',\n            {\n                'config_id': config.id,\n                'diagnostic_type': diagnostic_type,\n                'results_summary': {\n                    key: 'completed' for key in diagnostic_results.keys()\n                }\n            }\n        )\n        \n        response = Response({\n            'success': True,\n            'config_info': {\n                'id': config.id,\n                'nome_loja': config.nome_loja\n            },\n            'diagnostic_type': diagnostic_type,\n            'executed_at': timezone.now().isoformat(),\n            'results': diagnostic_results\n        })\n        \n        return SecurityHeadersManager.add_security_headers(response)\n        \n    except Exception as e:\n        logger.error(f\"Erro no diagnóstico manual: {e}\")\n        return Response({\n            'error': 'Erro interno do servidor'\n        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n\n# ===== FUNÇÕES AUXILIARES =====\n\ndef _aggregate_multi_store_stats(configs, period_hours):\n    \"\"\"Agrega estatísticas de múltiplas lojas\"\"\"\n    logging_service = get_structured_logging_service()\n    \n    aggregated = {\n        'total_stores': len(configs),\n        'period_analyzed': f\"{period_hours}h\",\n        'detection_summary': {\n            'total_attempts': 0,\n            'successful_detections': 0,\n            'failed_detections': 0,\n            'success_rate': 0\n        },\n        'method_effectiveness': {\n            'method_distribution': {},\n            'most_effective_methods': []\n        },\n        'performance_metrics': {\n            'average_response_time_ms': 0,\n            'total_processing_time_analyzed': 0\n        },\n        'quality_metrics': {\n            'confidence_distribution': {'alta': 0, 'media': 0, 'baixa': 0},\n            'error_count': 0,\n            'warning_count': 0,\n            'error_rate': 0\n        }\n    }\n    \n    store_stats = []\n    total_attempts = 0\n    total_successful = 0\n    total_processing_time = 0\n    processing_count = 0\n    \n    for config in configs:\n        try:\n            stats = logging_service.get_realtime_statistics(config.id, period_hours)\n            store_stats.append({\n                'config_id': config.id,\n                'nome_loja': config.nome_loja,\n                'stats': stats\n            })\n            \n            # Agrega números\n            detection = stats['detection_summary']\n            total_attempts += detection['total_attempts']\n            total_successful += detection['successful_detections']\n            \n            # Performance\n            perf = stats['performance_metrics']\n            if perf['average_response_time_ms'] > 0:\n                total_processing_time += perf['average_response_time_ms'] * detection['total_attempts']\n                processing_count += detection['total_attempts']\n            \n            # Qualidade\n            quality = stats['quality_metrics']\n            for level in ['alta', 'media', 'baixa']:\n                aggregated['quality_metrics']['confidence_distribution'][level] += \\\n                    quality['confidence_distribution'].get(level, 0)\n            \n            aggregated['quality_metrics']['error_count'] += quality['error_count']\n            aggregated['quality_metrics']['warning_count'] += quality['warning_count']\n            \n        except Exception as e:\n            logger.error(f\"Erro ao agregar stats da loja {config.id}: {e}\")\n    \n    # Calcula médias finais\n    if total_attempts > 0:\n        aggregated['detection_summary']['total_attempts'] = total_attempts\n        aggregated['detection_summary']['successful_detections'] = total_successful\n        aggregated['detection_summary']['failed_detections'] = total_attempts - total_successful\n        aggregated['detection_summary']['success_rate'] = (total_successful / total_attempts) * 100\n        \n        aggregated['quality_metrics']['error_rate'] = (\n            aggregated['quality_metrics']['error_count'] / total_attempts\n        ) * 100\n    \n    if processing_count > 0:\n        aggregated['performance_metrics']['average_response_time_ms'] = \\\n            total_processing_time / processing_count\n    \n    aggregated['store_breakdown'] = store_stats\n    \n    return aggregated\n\ndef _run_automated_diagnostic(configs, period_hours):\n    \"\"\"Executa diagnóstico automático\"\"\"\n    diagnostic = {\n        'timestamp': timezone.now().isoformat(),\n        'configs_analyzed': len(configs),\n        'period_hours': period_hours,\n        'issues_detected': [],\n        'warnings': [],\n        'recommendations': [],\n        'health_score': 100\n    }\n    \n    for config in configs:\n        try:\n            since = timezone.now() - timedelta(hours=period_hours)\n            \n            # Verifica logs de erro recentes\n            error_count = IPDetectionDebugLog.objects.filter(\n                config=config,\n                timestamp__gte=since,\n                nivel='ERROR'\n            ).count()\n            \n            if error_count > 10:\n                diagnostic['issues_detected'].append({\n                    'type': 'high_error_rate',\n                    'config': config.nome_loja,\n                    'details': f\"{error_count} erros nas últimas {period_hours}h\",\n                    'severity': 'high'\n                })\n                diagnostic['health_score'] -= 20\n            \n            # Verifica performance\n            avg_time = IPDetectionDebugLog.objects.filter(\n                config=config,\n                timestamp__gte=since,\n                tempo_processamento_ms__isnull=False\n            ).aggregate(avg_time=Avg('tempo_processamento_ms'))['avg_time']\n            \n            if avg_time and avg_time > 5000:\n                diagnostic['warnings'].append({\n                    'type': 'slow_performance',\n                    'config': config.nome_loja,\n                    'details': f\"Tempo médio de {avg_time:.0f}ms (>5s)\",\n                    'severity': 'medium'\n                })\n                diagnostic['health_score'] -= 10\n            \n            # Verifica taxa de sucesso\n            total_logs = IPDetectionDebugLog.objects.filter(\n                config=config,\n                timestamp__gte=since,\n                categoria='ip_detection'\n            ).count()\n            \n            success_logs = IPDetectionDebugLog.objects.filter(\n                config=config,\n                timestamp__gte=since,\n                categoria='ip_detection',\n                ip_detectado__isnull=False\n            ).count()\n            \n            if total_logs > 0:\n                success_rate = (success_logs / total_logs) * 100\n                if success_rate < 70:\n                    diagnostic['issues_detected'].append({\n                        'type': 'low_success_rate',\n                        'config': config.nome_loja,\n                        'details': f\"Taxa de sucesso: {success_rate:.1f}% (<70%)\",\n                        'severity': 'high'\n                    })\n                    diagnostic['health_score'] -= 15\n                    \n        except Exception as e:\n            logger.error(f\"Erro no diagnóstico da loja {config.id}: {e}\")\n    \n    # Gera recomendações baseadas nos problemas encontrados\n    if len(diagnostic['issues_detected']) == 0 and len(diagnostic['warnings']) == 0:\n        diagnostic['recommendations'].append(\"Sistema funcionando adequadamente\")\n    else:\n        if any(issue['type'] == 'high_error_rate' for issue in diagnostic['issues_detected']):\n            diagnostic['recommendations'].append(\"Investigar logs de erro detalhados\")\n        \n        if any(issue['type'] == 'low_success_rate' for issue in diagnostic['issues_detected']):\n            diagnostic['recommendations'].append(\"Revisar hierarquia de campos e qualidade dos dados\")\n            \n        if any(warning['type'] == 'slow_performance' for warning in diagnostic['warnings']):\n            diagnostic['recommendations'].append(\"Otimizar performance - verificar conectividade e cache\")\n    \n    return diagnostic\n\ndef _generate_automatic_recommendations(realtime_stats, dashboard_data):\n    \"\"\"Gera recomendações automáticas baseadas nas estatísticas\"\"\"\n    recommendations = {\n        'optimization': [],\n        'configuration': [],\n        'monitoring': [],\n        'data_quality': []\n    }\n    \n    try:\n        detection = realtime_stats.get('detection_summary', {})\n        performance = realtime_stats.get('performance_metrics', {})\n        quality = realtime_stats.get('quality_metrics', {})\n        \n        # Recomendações de otimização\n        if detection.get('success_rate', 0) < 80:\n            recommendations['optimization'].append(\n                \"Taxa de detecção baixa - considere ativar métodos alternativos de captura\"\n            )\n        \n        if performance.get('average_response_time_ms', 0) > 3000:\n            recommendations['optimization'].append(\n                \"Performance degradada - verificar conectividade e implementar cache\"\n            )\n        \n        # Recomendações de configuração\n        method_dist = realtime_stats.get('method_effectiveness', {}).get('method_distribution', {})\n        if 'client_details' not in method_dist or method_dist.get('client_details', 0) < 50:\n            recommendations['configuration'].append(\n                \"Método client_details não está sendo eficaz - revisar hierarquia de campos\"\n            )\n        \n        # Recomendações de monitoramento\n        if quality.get('error_rate', 0) > 15:\n            recommendations['monitoring'].append(\n                \"Taxa alta de erros - ativar alertas proativos\"\n            )\n        \n        # Recomendações de qualidade de dados\n        conf_dist = quality.get('confidence_distribution', {})\n        baixa_conf = conf_dist.get('baixa', 0)\n        total_conf = sum(conf_dist.values())\n        \n        if total_conf > 0 and (baixa_conf / total_conf) > 0.3:\n            recommendations['data_quality'].append(\n                \"Muitos IPs de baixa confiança - verificar qualidade dos dados de origem\"\n            )\n            \n    except Exception as e:\n        logger.error(f\"Erro ao gerar recomendações: {e}\")\n        recommendations['error'] = \"Erro ao gerar recomendações automáticas\"\n    \n    return recommendations\n\ndef _create_performance_timeline(debug_logs, hours):\n    \"\"\"Cria timeline de performance dividido em buckets\"\"\"\n    buckets = min(24, hours)  # Máximo 24 buckets\n    bucket_size_hours = hours / buckets\n    \n    timeline = []\n    now = timezone.now()\n    \n    for i in range(buckets):\n        bucket_end = now - timedelta(hours=i * bucket_size_hours)\n        bucket_start = now - timedelta(hours=(i + 1) * bucket_size_hours)\n        \n        bucket_logs = debug_logs.filter(\n            timestamp__gte=bucket_start,\n            timestamp__lt=bucket_end\n        )\n        \n        bucket_stats = bucket_logs.aggregate(\n            count=Count('id'),\n            avg_time=Avg('tempo_processamento_ms'),\n            success_count=Count('id', filter=Q(ip_detectado__isnull=False))\n        )\n        \n        timeline_point = {\n            'period_start': bucket_start.isoformat(),\n            'period_end': bucket_end.isoformat(),\n            'total_detections': bucket_stats['count'] or 0,\n            'average_time_ms': bucket_stats['avg_time'] or 0,\n            'successful_detections': bucket_stats['success_count'] or 0,\n            'success_rate': (\n                (bucket_stats['success_count'] / bucket_stats['count']) * 100\n                if bucket_stats['count'] else 0\n            )\n        }\n        \n        timeline.insert(0, timeline_point)  # Insere no início para ordem cronológica\n    \n    return timeline\n\ndef _generate_performance_insights(performance_stats, time_distribution):\n    \"\"\"Gera insights de performance\"\"\"\n    insights = []\n    \n    avg_time = performance_stats.get('avg_time', 0)\n    total_detections = performance_stats.get('total_detections', 0)\n    \n    if avg_time == 0:\n        insights.append(\"Nenhum dado de performance disponível no período\")\n        return insights\n    \n    # Análise de velocidade\n    if avg_time < 1000:\n        insights.append(f\"Excelente: Tempo médio muito baixo ({avg_time:.0f}ms)\")\n    elif avg_time < 3000:\n        insights.append(f\"Bom: Tempo médio aceitável ({avg_time:.0f}ms)\")\n    elif avg_time < 5000:\n        insights.append(f\"Atenção: Tempo médio alto ({avg_time:.0f}ms)\")\n    else:\n        insights.append(f\"Crítico: Tempo médio muito alto ({avg_time:.0f}ms)\")\n    \n    # Análise de distribuição\n    very_slow = time_distribution.get('very_slow', 0)\n    if very_slow > 0 and total_detections > 0:\n        slow_percentage = (very_slow / total_detections) * 100\n        if slow_percentage > 20:\n            insights.append(f\"Problema: {slow_percentage:.1f}% das detecções são muito lentas (>10s)\")\n        elif slow_percentage > 10:\n            insights.append(f\"Atenção: {slow_percentage:.1f}% das detecções são muito lentas (>10s)\")\n    \n    fast_count = time_distribution.get('very_fast', 0) + time_distribution.get('fast', 0)\n    if fast_count > 0 and total_detections > 0:\n        fast_percentage = (fast_count / total_detections) * 100\n        if fast_percentage > 80:\n            insights.append(f\"Ótimo: {fast_percentage:.1f}% das detecções são rápidas (<3s)\")\n    \n    return insights\n\ndef _run_performance_diagnostic(config):\n    \"\"\"Diagnóstico específico de performance\"\"\"\n    since_24h = timezone.now() - timedelta(hours=24)\n    since_7d = timezone.now() - timedelta(days=7)\n    \n    # Performance últimas 24h\n    recent_logs = IPDetectionDebugLog.objects.filter(\n        config=config,\n        timestamp__gte=since_24h,\n        tempo_processamento_ms__isnull=False\n    )\n    \n    recent_stats = recent_logs.aggregate(\n        count=Count('id'),\n        avg_time=Avg('tempo_processamento_ms'),\n        max_time=Max('tempo_processamento_ms'),\n        min_time=Min('tempo_processamento_ms')\n    )\n    \n    # Performance últimos 7 dias para comparação\n    week_logs = IPDetectionDebugLog.objects.filter(\n        config=config,\n        timestamp__gte=since_7d,\n        tempo_processamento_ms__isnull=False\n    )\n    \n    week_avg = week_logs.aggregate(avg_time=Avg('tempo_processamento_ms'))['avg_time'] or 0\n    \n    diagnostic = {\n        'recent_performance': recent_stats,\n        'week_average_ms': week_avg,\n        'performance_trend': 'stable',\n        'bottlenecks_detected': [],\n        'recommendations': []\n    }\n    \n    # Análise de tendência\n    recent_avg = recent_stats.get('avg_time', 0)\n    if recent_avg and week_avg:\n        if recent_avg > week_avg * 1.2:  # 20% pior\n            diagnostic['performance_trend'] = 'degrading'\n            diagnostic['bottlenecks_detected'].append('Performance piorou significativamente')\n        elif recent_avg < week_avg * 0.8:  # 20% melhor\n            diagnostic['performance_trend'] = 'improving'\n    \n    # Recomendações específicas\n    if recent_avg > 5000:\n        diagnostic['recommendations'].append('Implementar cache de resultados')\n        diagnostic['recommendations'].append('Verificar conectividade com API Shopify')\n    \n    if recent_stats.get('max_time', 0) > 15000:\n        diagnostic['recommendations'].append('Implementar timeout mais agressivo')\n    \n    return diagnostic\n\ndef _run_data_quality_diagnostic(config):\n    \"\"\"Diagnóstico específico de qualidade de dados\"\"\"\n    since_24h = timezone.now() - timedelta(hours=24)\n    \n    logs = IPDetectionDebugLog.objects.filter(\n        config=config,\n        timestamp__gte=since_24h\n    )\n    \n    # Análise de confiança\n    confidence_stats = {\n        'alta': logs.filter(score_confianca__gte=0.8).count(),\n        'media': logs.filter(score_confianca__gte=0.6, score_confianca__lt=0.8).count(),\n        'baixa': logs.filter(score_confianca__lt=0.6, score_confianca__isnull=False).count(),\n        'sem_score': logs.filter(score_confianca__isnull=True).count()\n    }\n    \n    # Análise de métodos\n    method_success = logs.filter(\n        categoria='ip_detection',\n        ip_detectado__isnull=False\n    ).values('metodo_deteccao').annotate(\n        count=Count('id')\n    ).order_by('-count')\n    \n    # Análise de erros\n    error_patterns = logs.filter(nivel='ERROR').values(\n        'subcategoria'\n    ).annotate(count=Count('id')).order_by('-count')[:5]\n    \n    total_attempts = logs.filter(categoria='ip_detection').count()\n    successful = logs.filter(categoria='ip_detection', ip_detectado__isnull=False).count()\n    \n    diagnostic = {\n        'confidence_distribution': confidence_stats,\n        'method_effectiveness': list(method_success),\n        'error_patterns': list(error_patterns),\n        'overall_quality': {\n            'total_attempts': total_attempts,\n            'successful_detections': successful,\n            'success_rate': (successful / total_attempts * 100) if total_attempts else 0\n        },\n        'data_issues': [],\n        'recommendations': []\n    }\n    \n    # Identifica problemas\n    if confidence_stats['baixa'] > confidence_stats['alta']:\n        diagnostic['data_issues'].append('Muitos IPs de baixa confiança detectados')\n        diagnostic['recommendations'].append('Revisar hierarquia de campos de IP')\n    \n    if diagnostic['overall_quality']['success_rate'] < 70:\n        diagnostic['data_issues'].append('Taxa de sucesso baixa')\n        diagnostic['recommendations'].append('Verificar qualidade dos dados de entrada')\n    \n    return diagnostic\n\ndef _run_system_health_diagnostic(config):\n    \"\"\"Diagnóstico de saúde geral do sistema\"\"\"\n    # Verifica alertas ativos\n    active_alerts = IPDetectionAlert.objects.filter(\n        config=config,\n        status='active'\n    ).count()\n    \n    # Verifica últimas estatísticas\n    latest_stat = IPDetectionStatistics.objects.filter(\n        config=config\n    ).order_by('-data_referencia').first()\n    \n    # Verifica logs recentes\n    recent_activity = IPDetectionDebugLog.objects.filter(\n        config=config,\n        timestamp__gte=timezone.now() - timedelta(hours=1)\n    ).count()\n    \n    diagnostic = {\n        'active_alerts_count': active_alerts,\n        'latest_statistics': {\n            'date': latest_stat.data_referencia.isoformat() if latest_stat else None,\n            'success_rate': latest_stat.taxa_sucesso_geral if latest_stat else 0\n        },\n        'recent_activity': recent_activity,\n        'system_status': 'healthy',\n        'concerns': []\n    }\n    \n    # Determina status geral\n    if active_alerts > 5:\n        diagnostic['system_status'] = 'attention_needed'\n        diagnostic['concerns'].append(f'{active_alerts} alertas ativos')\n    \n    if latest_stat and latest_stat.taxa_sucesso_geral < 70:\n        diagnostic['system_status'] = 'attention_needed'\n        diagnostic['concerns'].append('Taxa de sucesso recente baixa')\n    \n    if recent_activity == 0:\n        diagnostic['concerns'].append('Nenhuma atividade recente detectada')\n    \n    return diagnostic\n\ndef _generate_diagnostic_recommendations(diagnostic_results):\n    \"\"\"Gera recomendações baseadas em diagnóstico completo\"\"\"\n    recommendations = {\n        'immediate_actions': [],\n        'optimization_opportunities': [],\n        'monitoring_improvements': [],\n        'long_term_strategies': []\n    }\n    \n    # Analisa cada área de diagnóstico\n    perf = diagnostic_results.get('performance', {})\n    quality = diagnostic_results.get('data_quality', {})\n    health = diagnostic_results.get('system_health', {})\n    \n    # Ações imediatas\n    if perf.get('performance_trend') == 'degrading':\n        recommendations['immediate_actions'].append(\n            'Investigar degradação de performance recente'\n        )\n    \n    if health.get('active_alerts_count', 0) > 5:\n        recommendations['immediate_actions'].append(\n            'Revisar e resolver alertas ativos'\n        )\n    \n    # Oportunidades de otimização\n    if quality.get('overall_quality', {}).get('success_rate', 0) < 80:\n        recommendations['optimization_opportunities'].append(\n            'Otimizar hierarquia de detecção de IP'\n        )\n    \n    # Melhorias de monitoramento\n    recommendations['monitoring_improvements'].append(\n        'Configurar alertas proativos baseados em thresholds'\n    )\n    \n    # Estratégias de longo prazo\n    recommendations['long_term_strategies'].append(\n        'Implementar machine learning para detecção preditiva'\n    )\n    \n    return recommendations