# backend/features/processamento/views.py
from rest_framework import status
from rest_framework.decorators import api_view, permission_classes
from rest_framework.permissions import IsAuthenticated
from rest_framework.response import Response
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator
from django.views.decorators.cache import never_cache
from django.http import JsonResponse
from django.core.cache import cache
from django.utils import timezone
import json
import logging
import requests
from datetime import datetime, timedelta
from collections import defaultdict
from requests.exceptions import ConnectionError, Timeout, HTTPError, RequestException
import django_rq
from django_rq import get_queue
import ipaddress
import re

from .models import ShopifyConfig, ProcessamentoLog
from .services.shopify_detector import ShopifyDuplicateOrderDetector
from .services.improved_ip_detection import get_improved_ip_detector
from .services.alternative_ip_capture import AlternativeIPCaptureService
from .services.geolocation_api_service import get_geolocation_service, GeolocationConfig
from .services.enhanced_ip_detector import get_enhanced_ip_detector
from .services.structured_logging_service import get_structured_logging_service
from .cache_manager import get_cache_manager
from .utils.security_utils import (
    IPSecurityUtils, 
    RateLimitManager, 
    AuditLogger, 
    SecurityHeadersManager
)

logger = logging.getLogger(__name__)

def validate_ip_address(ip):
    """Valida formato de endere√ßo IP"""
    try:
        # Remove caracteres n√£o permitidos
        ip_clean = re.sub(r'[^0-9.:a-fA-F]', '', str(ip))
        # Valida formato
        ipaddress.ip_address(ip_clean)
        return ip_clean
    except (ValueError, TypeError):
        raise ValueError('Formato de IP inv√°lido')

def create_safe_log(user, config, tipo, status, dados=None):
    """
    Cria log de forma segura, verificando se o usu√°rio est√° autenticado
    """
    try:
        # Verifica se o usu√°rio √© v√°lido e autenticado
        if hasattr(user, 'is_authenticated') and user.is_authenticated:
            log_user = user
        else:
            # Se n√£o h√° usu√°rio autenticado, usa None (pode ser um request an√¥nimo)
            log_user = None
        
        ProcessamentoLog.objects.create(
            user=log_user,
            config=config,
            tipo=tipo,
            status=status,
            detalhes=dados or {}
        )
    except Exception as e:
        # Log do erro mas n√£o interrompe a execu√ß√£o principal
        logger.error(f"Erro ao criar log: {str(e)}")

@csrf_exempt
@api_view(['GET', 'POST', 'DELETE'])
def lojas_config(request):
    """Gerencia m√∫ltiplas configura√ß√µes do Shopify"""
    if request.method == 'GET':
        try:
            configs = ShopifyConfig.objects.filter(ativo=True)
            lojas_data = []
            for config in configs:
                lojas_data.append({
                    'id': config.id,
                    'nome_loja': config.nome_loja,
                    'shop_url': config.shop_url,
                    'api_version': config.api_version,
                    'data_criacao': config.data_criacao.isoformat(),
                    'criador': config.user.username
                })
            return Response({'lojas': lojas_data})
        except Exception as e:
            return Response({'error': str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    elif request.method == 'POST':
        try:
            nome_loja = request.data.get('nome_loja', '').strip()
            shop_url = request.data.get('shop_url', '').strip()
            access_token = request.data.get('access_token', '').strip()
            
            if not nome_loja or not shop_url or not access_token:
                return Response({'error': 'Nome da loja, URL e token s√£o obrigat√≥rios'}, status=status.HTTP_400_BAD_REQUEST)
            
            # Remove protocolo da URL se presente
            shop_url = shop_url.replace('https://', '').replace('http://', '')
            
            # Verifica se j√° existe loja com mesma URL
            if ShopifyConfig.objects.filter(shop_url=shop_url, ativo=True).exists():
                return Response({'error': 'J√° existe uma loja com esta URL'}, status=status.HTTP_400_BAD_REQUEST)
            
            # Testa conex√£o
            detector = ShopifyDuplicateOrderDetector(shop_url, access_token)
            connection_ok, message = detector.test_connection()
            
            if not connection_ok:
                return Response({'error': f'Falha na conex√£o: {message}'}, status=status.HTTP_400_BAD_REQUEST)
            
            # Cria nova configura√ß√£o
            config = ShopifyConfig.objects.create(
                user=request.user,
                nome_loja=nome_loja,
                shop_url=shop_url,
                access_token=access_token
            )
            
            return Response({
                'message': f'Loja {nome_loja} adicionada com sucesso!',
                'loja': {
                    'id': config.id,
                    'nome_loja': config.nome_loja,
                    'shop_url': config.shop_url
                }
            })
            
        except Exception as e:
            return Response({'error': str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    elif request.method == 'DELETE':
        try:
            loja_id = request.data.get('loja_id')
            if not loja_id:
                return Response({'error': 'ID da loja √© obrigat√≥rio'}, status=status.HTTP_400_BAD_REQUEST)
            
            config = ShopifyConfig.objects.filter(id=loja_id).first()
            if not config:
                return Response({'error': 'Loja n√£o encontrada'}, status=status.HTTP_404_NOT_FOUND)
            
            nome_loja = config.nome_loja
            config.delete()
            
            return Response({'message': f'Loja {nome_loja} removida com sucesso!'})
            
        except Exception as e:
            return Response({'error': str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

@csrf_exempt
@api_view(['POST'])
def test_connection(request):
    """Testa conex√£o com Shopify"""
    try:
        shop_url = request.data.get('shop_url', '').strip()
        access_token = request.data.get('access_token', '').strip()
        
        if not shop_url or not access_token:
            return Response({'error': 'URL da loja e token s√£o obrigat√≥rios'}, status=status.HTTP_400_BAD_REQUEST)
        
        shop_url = shop_url.replace('https://', '').replace('http://', '')
        
        detector = ShopifyDuplicateOrderDetector(shop_url, access_token)
        connection_ok, message = detector.test_connection()
        
        return Response({
            'success': connection_ok,
            'message': message
        })
        
    except Exception as e:
        return Response({'error': str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

@api_view(['POST'])
@permission_classes([IsAuthenticated])
def buscar_duplicatas(request):
    """Busca pedidos duplicados de uma loja espec√≠fica"""
    try:
        loja_id = request.data.get('loja_id')
        
        # Log de auditoria
        logger.info(f"Busca de duplicatas - Usu√°rio: {request.user.username} (ID: {request.user.id}), Loja: {loja_id}")
        
        if not loja_id:
            logger.warning(f"‚ùå ID da loja n√£o fornecido")
            return Response({'error': 'ID da loja √© obrigat√≥rio'}, status=status.HTTP_400_BAD_REQUEST)
        
        # Debug: Verificar todas as lojas ativas (n√£o limitadas por usu√°rio)
        available_configs = ShopifyConfig.objects.filter(ativo=True)
        logger.info(f"üìä Todas as lojas ativas: {[(c.id, c.nome_loja, c.user.username) for c in available_configs]}")
        
        # CORRE√á√ÉO: Remover filtro por usu√°rio - qualquer usu√°rio pode acessar qualquer loja ativa
        config = ShopifyConfig.objects.filter(id=loja_id, ativo=True).first()
        if not config:
            logger.warning(f"‚ùå Loja {loja_id} n√£o encontrada ou inativa")
            return Response({'error': 'Loja n√£o encontrada ou inativa', 'debug_info': {
                'requested_loja_id': loja_id,
                'available_lojas': [(c.id, c.nome_loja) for c in available_configs],
                'total_active_stores': available_configs.count()
            }}, status=status.HTTP_400_BAD_REQUEST)
        
        logger.info(f"‚úÖ Loja encontrada: {config.nome_loja} (ID: {config.id})")
        
        detector = ShopifyDuplicateOrderDetector(config.shop_url, config.access_token)
        duplicates = detector.get_detailed_duplicates()
        
        # Log da busca
        create_safe_log(
            user=request.user,
            config=config,
            tipo='busca',
            status='sucesso',
            dados={
                'duplicates_count': len(duplicates),
                'pedidos_encontrados': len(duplicates)
            }
        )
        
        return Response({
            'duplicates': duplicates,
            'count': len(duplicates),
            'loja_nome': config.nome_loja
        })
        
    except Exception as e:
        # Log do erro com mais detalhes
        logger.error(f"‚ùå Erro na busca de duplicatas: {str(e)}", exc_info=True)
        logger.error(f"üìã Request data: {request.data}")
        logger.error(f"üë§ User: {request.user}")
        
        if 'config' in locals():
            ProcessamentoLog.objects.create(
                user=request.user,
                config=config,
                tipo='busca',
                status='erro',
                erro_mensagem=str(e)
            )
        
        return Response({'error': str(e), 'debug_info': {
            'loja_id': request.data.get('loja_id'),
            'user_id': request.user.id,
            'user_username': request.user.username
        }}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

@api_view(['POST'])
@permission_classes([IsAuthenticated])
def cancelar_pedido(request):
    """Cancela um pedido espec√≠fico"""
    try:
        loja_id = request.data.get('loja_id')
        order_id = request.data.get('order_id')
        order_number = request.data.get('order_number', 'N/A')
        
        # Log de auditoria
        logger.info(f"Cancelamento de pedido {order_id} - Usu√°rio: {request.user.username} (ID: {request.user.id}), Loja: {loja_id}")
        
        if not loja_id or not order_id:
            return Response({'error': 'ID da loja e do pedido s√£o obrigat√≥rios'}, status=status.HTTP_400_BAD_REQUEST)
        
        config = ShopifyConfig.objects.filter(id=loja_id, ativo=True, user=request.user).first()
        if not config:
            return Response({'error': 'Loja n√£o encontrada'}, status=status.HTTP_400_BAD_REQUEST)
        
        detector = ShopifyDuplicateOrderDetector(config.shop_url, config.access_token)
        success, message = detector.cancel_order(order_id)
        
        # Log da opera√ß√£o
        log_status = 'sucesso' if success else 'erro'
        ProcessamentoLog.objects.create(
            user=request.user,
            config=config,
            tipo='cancelamento',
            status=log_status,
            pedidos_cancelados=1 if success else 0,
            detalhes={'order_id': order_id, 'order_number': order_number},
            erro_mensagem=message if not success else ''
        )
        
        return Response({
            'success': success,
            'message': message
        })
        
    except Exception as e:
        return Response({'error': str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

@api_view(['POST'])
@permission_classes([IsAuthenticated])
def cancelar_lote(request):
    """Cancela m√∫ltiplos pedidos"""
    try:
        loja_id = request.data.get('loja_id')
        order_ids = request.data.get('order_ids', [])
        
        # Log de auditoria
        logger.info(f"Cancelamento em lote de {len(order_ids)} pedidos - Usu√°rio: {request.user.username} (ID: {request.user.id}), Loja: {loja_id}")
        
        if not loja_id or not order_ids:
            return Response({'error': 'ID da loja e lista de pedidos s√£o obrigat√≥rios'}, status=status.HTTP_400_BAD_REQUEST)
        
        config = ShopifyConfig.objects.filter(id=loja_id, ativo=True, user=request.user).first()
        if not config:
            return Response({'error': 'Loja n√£o encontrada'}, status=status.HTTP_400_BAD_REQUEST)
        
        detector = ShopifyDuplicateOrderDetector(config.shop_url, config.access_token)
        
        results = []
        success_count = 0
        
        for order_id in order_ids:
            success, message = detector.cancel_order(order_id)
            results.append({
                'order_id': order_id,
                'success': success,
                'message': message
            })
            
            if success:
                success_count += 1
        
        # Log da opera√ß√£o em lote
        log_status = 'sucesso' if success_count == len(order_ids) else ('parcial' if success_count > 0 else 'erro')
        ProcessamentoLog.objects.create(
            user=request.user,
            config=config,
            tipo='cancelamento_lote',
            status=log_status,
            pedidos_encontrados=len(order_ids),
            pedidos_cancelados=success_count,
            detalhes={'results': results}
        )
        
        return Response({
            'results': results,
            'success_count': success_count,
            'total_count': len(order_ids)
        })
        
    except Exception as e:
        return Response({'error': str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

@api_view(['POST'])
@permission_classes([IsAuthenticated])
@never_cache
def buscar_pedidos_mesmo_ip(request):
    """Busca pedidos agrupados pelo mesmo IP com medidas de seguran√ßa - VERS√ÉO CORRIGIDA"""
    try:
        # === LOG INICIAL PARA DEBUG ===
        logger.info(f"=== IN√çCIO buscar_pedidos_mesmo_ip ===")
        logger.info(f"User: {getattr(request.user, 'username', 'Anonymous')}")
        logger.info(f"Request data: {request.data}")
        
        # === VALIDA√á√ïES DE SEGURAN√áA ===
        loja_id = request.data.get('loja_id')
        days = request.data.get('days', 30)
        min_orders = request.data.get('min_orders', 2)
        
        logger.info(f"Par√¢metros recebidos - loja_id: {loja_id}, days: {days}, min_orders: {min_orders}")
        
        # Valida√ß√£o obrigat√≥ria de par√¢metros
        if not loja_id:
            logger.error("loja_id n√£o fornecido")
            return Response({'error': 'ID da loja √© obrigat√≥rio'}, status=status.HTTP_400_BAD_REQUEST)
        
        # Sanitiza√ß√£o e valida√ß√£o de inputs
        try:
            loja_id = int(loja_id)
            days = int(days)  # Remove limite artificial
            min_orders = max(int(min_orders), 2)
            logger.info(f"Par√¢metros validados - loja_id: {loja_id}, days: {days}, min_orders: {min_orders}")
        except (ValueError, TypeError) as param_error:
            logger.error(f"Erro de valida√ß√£o de par√¢metros: {str(param_error)}")
            return Response({'error': 'Par√¢metros inv√°lidos'}, status=status.HTTP_400_BAD_REQUEST)
        
        # Implementa limite din√¢mico - ser√° calculado ap√≥s obter configura√ß√£o da loja
        # Valida√ß√£o tempor√°ria para evitar abuso
        if days > 365:
            logger.warning(f"Per√≠odo muito alto solicitado: {days} dias")
            return Response({
                'error': 'Per√≠odo m√°ximo absoluto √© 365 dias',
                'details': 'Use um per√≠odo menor para melhor performance'
            }, status=status.HTTP_400_BAD_REQUEST)
        
        if days < 1:
            days = 1
            logger.info(f"Per√≠odo ajustado para minimum: {days}")
        
        # Busca configura√ß√£o da loja (apenas do usu√°rio autenticado)
        try:
            logger.info(f"Buscando configura√ß√£o da loja {loja_id} para usu√°rio {request.user.username}")
            config = ShopifyConfig.objects.filter(id=loja_id, ativo=True, user=request.user).first()
            if not config:
                logger.warning(f"Loja {loja_id} n√£o encontrada ou inativa")
                # Lista lojas dispon√≠veis para debug
                all_configs = ShopifyConfig.objects.all()
                logger.warning(f"Lojas no banco: {[(c.id, c.nome_loja, c.ativo) for c in all_configs]}")
                return Response({
                    'error': 'Loja n√£o encontrada ou inativa',
                    'details': 'Configure uma loja Shopify v√°lida antes de usar esta funcionalidade',
                    'action_required': 'add_shopify_store'
                }, status=status.HTTP_404_NOT_FOUND)
                
            logger.info(f"Loja encontrada: {config.nome_loja}")
            
            # IMPLEMENTA LIMITE DIN√ÇMICO baseado no volume da loja
            try:
                logger.info("Calculando limite din√¢mico...")
                max_days_allowed = _calculate_dynamic_limit_for_store(config)
                logger.info(f"Limite din√¢mico calculado: {max_days_allowed} dias")
                
                if days > max_days_allowed:
                    logger.warning(f"Per√≠odo {days} dias excede limite din√¢mico {max_days_allowed} para loja {config.nome_loja}")
                    return Response({
                        'error': f'Per√≠odo m√°ximo permitido √© {max_days_allowed} dias para a loja "{config.nome_loja}"',
                        'details': f'Limite baseado no volume de pedidos e capacidade de processamento da loja',
                        'suggested_period': min(days, max_days_allowed),
                        'loja_volume_category': _get_store_volume_category(config),
                        'current_limit': max_days_allowed,
                        'requested_period': days
                    }, status=status.HTTP_400_BAD_REQUEST)
            except Exception as limit_error:
                logger.error(f"Erro ao calcular limite din√¢mico: {str(limit_error)}", exc_info=True)
                # Usar limite padr√£o se falhar
                max_days_allowed = 30
                if days > max_days_allowed:
                    return Response({
                        'error': f'Per√≠odo m√°ximo permitido √© {max_days_allowed} dias (limite padr√£o)',
                        'details': 'Erro na avalia√ß√£o de limite din√¢mico'
                    }, status=status.HTTP_400_BAD_REQUEST)
                
        except Exception as db_error:
            logger.error(f"Erro de banco de dados ao buscar loja {loja_id}: {str(db_error)}", exc_info=True)
            return Response({'error': 'Erro de banco de dados'}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        
        # Testa conex√£o com Shopify antes de prosseguir
        try:
            logger.info("Criando detector Shopify...")
            detector = ShopifyDuplicateOrderDetector(config.shop_url, config.access_token)
            logger.info(f"Detector criado. Testando conex√£o Shopify para loja {config.nome_loja}")
            
            connection_ok, test_message = detector.test_connection()
            logger.info(f"Resultado do teste de conex√£o: {connection_ok}, mensagem: {test_message}")
            
            if not connection_ok:
                logger.error(f"Falha na conex√£o Shopify para loja {config.nome_loja}: {test_message}")
                return Response({
                    'error': 'Erro de autentica√ß√£o com Shopify',
                    'details': f'A conex√£o com a loja "{config.nome_loja}" falhou: {test_message}',
                    'action_required': 'update_shopify_credentials',
                    'loja_nome': config.nome_loja
                }, status=status.HTTP_401_UNAUTHORIZED)
            
            logger.info(f"Conex√£o Shopify OK para loja {config.nome_loja}")
            
        except Exception as connection_error:
            logger.error(f"ERRO CR√çTICO ao testar conex√£o Shopify: {str(connection_error)}", exc_info=True)
            return Response({
                'error': 'Erro de conectividade com Shopify',
                'details': f'N√£o foi poss√≠vel conectar com a API do Shopify: {str(connection_error)}',
                'action_required': 'check_connectivity'
            }, status=status.HTTP_503_SERVICE_UNAVAILABLE)
        
        # Buscar dados de IP com tratamento de erro melhorado e otimiza√ß√µes de timeout
        try:
            logger.info(f"Buscando pedidos por IP - days: {days}, min_orders: {min_orders}")
            
            # APLICA CACHE E OTIMIZA√á√ïES PARA PREVEN√á√ÉO DE TIMEOUT
            # Primeiro verifica cache
            cache_manager = get_cache_manager()
            cached_result = cache_manager.get_ip_search_results(loja_id, days, min_orders)
            
            if cached_result:
                logger.info(f"Cache HIT para busca IP - loja: {loja_id}, days: {days}, min_orders: {min_orders}")
                ip_data = cached_result['data']
                # Adiciona flag de cache na resposta
                ip_data['from_cache'] = True
                ip_data['cache_timestamp'] = cached_result.get('timestamp', '')
            else:
                logger.info(f"Cache MISS - executando busca real")
                
                # IMPLEMENTA LIMITE DIN√ÇMICO OTIMIZADO PARA EVITAR 499
                if days > 30:
                    logger.warning(f"Per√≠odo {days} dias muito alto - usando processamento ass√≠ncrono")
                    # Para per√≠odos muito longos, usa job ass√≠ncrono
                    return _handle_async_ip_search(request, detector, config, days, min_orders)
                else:
                    # Per√≠odo aceit√°vel - usa m√©todo otimizado com timeout reduzido
                    ip_data = _get_optimized_ip_data(detector, days, min_orders)
                    
                    # Salva no cache se bem sucedido
                    if ip_data and 'error' not in ip_data:
                        cache_manager.cache_ip_search_results(loja_id, days, min_orders, {'data': ip_data})
                        logger.info(f"Resultado salvo no cache - TTL: 10 minutos")
            
            logger.info(f"Busca por IP conclu√≠da - IPs encontrados: {ip_data.get('total_ips_found', 0)}")
            
        except HTTPError as http_error:
            # Log seguro com informa√ß√µes t√©cnicas detalhadas
            logger.error(
                f"Erro HTTP na busca por IP - User: {getattr(request.user, 'username', 'Anonymous')}, "
                f"Status: {getattr(http_error.response, 'status_code', 'N/A')}, "
                f"Error: {str(http_error)}",
                exc_info=True
            )
            
            # Tratamento espec√≠fico por c√≥digo HTTP MELHORADO
            if hasattr(http_error, 'response') and hasattr(http_error.response, 'status_code'):
                status_code = http_error.response.status_code
                
                if status_code == 401:
                    return Response({
                        'error': 'Erro de autentica√ß√£o com Shopify. Verifique o token de acesso da loja.',
                        'details': 'Token de acesso inv√°lido ou expirado',
                        'error_code': 'SHOPIFY_AUTH_ERROR'
                    }, status=status.HTTP_401_UNAUTHORIZED)
                elif status_code == 403:
                    return Response({
                        'error': 'Acesso negado pela API do Shopify. Verifique as permiss√µes do token.',
                        'details': 'Token n√£o possui permiss√µes necess√°rias para acessar pedidos',
                        'error_code': 'SHOPIFY_PERMISSION_ERROR'
                    }, status=status.HTTP_403_FORBIDDEN)
                elif status_code == 429:
                    # Rate limit espec√≠fico
                    return Response({
                        'error': 'Limite de requisi√ß√µes do Shopify atingido',
                        'details': 'Aguarde alguns minutos antes de tentar novamente',
                        'error_code': 'SHOPIFY_RATE_LIMIT',
                        'retry_after_seconds': 60
                    }, status=status.HTTP_429_TOO_MANY_REQUESTS)
                elif status_code == 504 or status_code == 502:
                    # Gateway timeout ou bad gateway
                    return Response({
                        'error': 'Timeout na API do Shopify',
                        'details': 'A requisi√ß√£o demorou muito para ser processada. Tente com um per√≠odo menor.',
                        'error_code': 'SHOPIFY_TIMEOUT',
                        'suggested_action': 'reduce_period'
                    }, status=status.HTTP_503_SERVICE_UNAVAILABLE)
            
            return Response({
                'error': 'Erro ao buscar dados de IP no Shopify',
                'details': 'Entre em contato com o suporte t√©cnico',
                'error_code': 'SHOPIFY_API_ERROR'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        
        except (ConnectionError, Timeout) as conn_error:
            logger.error(f"Erro de conectividade na busca por IP: {str(conn_error)}", exc_info=True)
            
            # Determina se √© timeout ou erro de conex√£o
            if 'timeout' in str(conn_error).lower() or isinstance(conn_error, Timeout):
                return Response({
                    'error': 'Timeout na busca por IPs',
                    'details': f'A busca por {days} dias demorou muito para ser processada. Tente com um per√≠odo menor.',
                    'error_code': 'REQUEST_TIMEOUT',
                    'suggested_period': min(days // 2, 30),
                    'current_period': days
                }, status=status.HTTP_503_SERVICE_UNAVAILABLE)
            else:
                return Response({
                    'error': 'Problema de conectividade com Shopify',
                    'details': 'Verifique sua conex√£o de internet e tente novamente',
                    'error_code': 'CONNECTION_ERROR'
                }, status=status.HTTP_503_SERVICE_UNAVAILABLE)
        
        except RequestException as req_error:
            logger.error(f"Erro de requisi√ß√£o na busca por IP: {str(req_error)}", exc_info=True)
            return Response({
                'error': 'Erro ao buscar dados de IP no Shopify',
                'details': 'Entre em contato com o suporte t√©cnico'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        
        except Exception as search_error:
            # Log de erro gen√©rico com informa√ß√µes t√©cnicas detalhadas
            logger.error(
                f"Erro inesperado na busca por IP - User: {getattr(request.user, 'username', 'Anonymous')}, "
                f"Type: {type(search_error).__name__}, Error: {str(search_error)}",
                exc_info=True
            )
            return Response({
                'error': 'Erro interno no processamento',
                'details': f'Erro espec√≠fico: {str(search_error)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        
        # === AUDITORIA DE SEGURAN√áA ===
        try:
            audit_details = {
                'ips_found': ip_data.get('total_ips_found', 0),
                'period_days': days,
                'min_orders': min_orders,
                'total_orders': ip_data.get('total_orders_analyzed', 0)
            }
            
            # Log padr√£o da busca
            ProcessamentoLog.objects.create(
                user=request.user,
                config=config,
                tipo='busca_ip',
                status='sucesso',
                pedidos_encontrados=ip_data.get('total_orders_analyzed', 0),
                detalhes=audit_details
            )
            
            logger.info(f"Busca por IP conclu√≠da com sucesso - User: {getattr(request.user, 'username', 'Anonymous')}")
            
        except Exception as audit_error:
            logger.error(f"Erro na auditoria (n√£o cr√≠tico): {str(audit_error)}")
        
        # Prepara dados de resposta
        response = Response({
            'success': True,
            'data': ip_data,
            'loja_nome': config.nome_loja
        })
        
        return response
        
    except Exception as e:
        # === LOG DE ERRO COM STACK TRACE COMPLETO ===
        logger.error(f"ERRO CR√çTICO em buscar_pedidos_mesmo_ip - User: {getattr(request.user, 'username', 'Anonymous') if hasattr(request, 'user') else 'Unknown'}, Error: {str(e)}", exc_info=True)
        
        # Log padr√£o do erro
        try:
            if 'config' in locals():
                ProcessamentoLog.objects.create(
                    user=request.user,
                    config=config,
                    tipo='busca_ip',
                    status='erro',
                    erro_mensagem=str(e)
                )
        except Exception as log_error:
            logger.error(f"Erro ao salvar log de erro: {str(log_error)}")
        
        return Response({
            'error': 'Erro interno do servidor', 
            'details': f'Erro espec√≠fico: {str(e)}'
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

@api_view(['POST'])
@permission_classes([IsAuthenticated])
@never_cache
def detalhar_pedidos_ip(request):
    """Retorna dados detalhados dos clientes de um IP espec√≠fico usando dados reais do Shopify - VERS√ÉO CORRIGIDA"""
    try:
        # === VALIDA√á√ïES B√ÅSICAS ===
        loja_id = request.data.get('loja_id')
        ip = request.data.get('ip')
        days = request.data.get('days', 30)  # Permite configurar per√≠odo
        
        # Valida√ß√£o de IP
        try:
            ip = validate_ip_address(ip) if ip else None
        except ValueError:
            return Response({'error': 'Formato de IP inv√°lido'}, status=status.HTTP_400_BAD_REQUEST)
        
        # Log de auditoria
        logger.info(f"Acesso a dados de IP {ip} por usu√°rio {request.user.username} (ID: {request.user.id})")
        
        if not loja_id or not ip:
            return Response({'error': 'ID da loja e IP s√£o obrigat√≥rios'}, status=status.HTTP_400_BAD_REQUEST)
        
        # Sanitiza√ß√£o de par√¢metros
        try:
            loja_id = int(loja_id)
            # ‚ö° CORRE√á√ÉO CR√çTICA: Usa mesmo limite que buscar_ips_duplicados_simples (90 dias)
            days = min(int(days), 90)  # M√°ximo 90 dias (consistente com busca principal)
        except (ValueError, TypeError) as param_error:
            logger.error(f"Erro de valida√ß√£o de par√¢metros: {str(param_error)}")
            return Response({'error': 'Par√¢metros inv√°lidos'}, status=status.HTTP_400_BAD_REQUEST)
        
        # === VERIFICA√á√ÉO DE CACHE ESTRAT√âGICO ===
        cache_manager = get_cache_manager()
        cached_result = cache_manager.get_ip_details(loja_id, ip, days)
        
        if cached_result:
            logger.info(f"Cache HIT para IP {ip} (loja: {loja_id}, days: {days}) - retornando dados em cache")
            return Response({
                'success': True,
                'data': cached_result.get('data', {}),
                'cached': True,
                'cache_timestamp': cached_result.get('timestamp', ''),
                'message': 'Dados retornados do cache para otimizar performance'
            })
        
        # Busca configura√ß√£o da loja (qualquer loja ativa no sistema)
        try:
            config = ShopifyConfig.objects.filter(id=loja_id, ativo=True).first()
            if not config:
                logger.warning(f"Loja {loja_id} n√£o encontrada ou inativa")
                return Response({
                    'error': 'Loja n√£o encontrada ou inativa',
                    'details': 'Configure uma loja Shopify v√°lida antes de usar esta funcionalidade',
                    'action_required': 'add_shopify_store'
                }, status=status.HTTP_404_NOT_FOUND)
        except Exception as db_error:
            logger.error(f"Erro de banco de dados ao buscar loja {loja_id}: {str(db_error)}")
            return Response({'error': 'Erro de banco de dados'}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        
        # Testa conex√£o com Shopify antes de prosseguir
        try:
            detector = ShopifyDuplicateOrderDetector(config.shop_url, config.access_token)
            logger.info(f"Testando conex√£o Shopify para loja {config.nome_loja}")
            
            connection_ok, test_message = detector.test_connection()
            if not connection_ok:
                logger.error(f"Falha na conex√£o Shopify para loja {config.nome_loja}: {test_message}")
                return Response({
                    'error': 'Erro de autentica√ß√£o com Shopify',
                    'details': f'A conex√£o com a loja "{config.nome_loja}" falhou: {test_message}',
                    'action_required': 'update_shopify_credentials',
                    'loja_nome': config.nome_loja
                }, status=status.HTTP_401_UNAUTHORIZED)
            
            logger.info(f"Conex√£o Shopify OK para loja {config.nome_loja}")
            
        except Exception as connection_error:
            logger.error(f"Erro ao testar conex√£o Shopify: {str(connection_error)}", exc_info=True)
            return Response({
                'error': 'Erro de conectividade com Shopify',
                'details': 'N√£o foi poss√≠vel conectar com a API do Shopify. Verifique sua conex√£o de internet e configura√ß√µes.',
                'action_required': 'check_connectivity'
            }, status=status.HTTP_503_SERVICE_UNAVAILABLE)
        
        # === BUSCA DADOS REAIS DO SHOPIFY - M√âTODO ULTRA R√ÅPIDO ===
        try:
            logger.info(f"Buscando detalhes para IP {ip} - days: {days} (M√âTODO OTIMIZADO)")
            
            # Usa m√©todo espec√≠fico para IP √∫nico - busca TODOS os pedidos do IP
            specific_orders = detector.get_orders_for_specific_ip(
                target_ip=ip,
                days=days,
                max_orders=200  # Aumentado para garantir que pega TODOS os pedidos do IP
            )
            
            if not specific_orders:
                logger.info(f"Nenhum pedido encontrado para IP {ip}")
                return Response({
                    'success': True,
                    'data': {
                        'ip': ip,
                        'total_orders': 0,
                        'client_details': [],
                        'active_orders': 0,
                        'cancelled_orders': 0
                    },
                    'loja_nome': config.nome_loja,
                    'message': f'Nenhum pedido encontrado para o IP {ip} nos √∫ltimos {days} dias'
                })
            
            logger.info(f"Encontrados {len(specific_orders)} pedidos para IP {ip}")
            
            # ‚ö° APLICA MESMO FILTRO DA TABELA: S√≥ pedidos de clientes DIFERENTES
            clientes_unicos = set()
            pedidos_filtrados = []
            
            # Primeiro, identifica todos os clientes √∫nicos
            for order in specific_orders:
                customer = order.get('customer', {})
                customer_name = f"{customer.get('first_name', '')} {customer.get('last_name', '')}".strip()
                if customer_name:
                    clientes_unicos.add(customer_name)
            
            # DEBUG: Log detalhado para comparar com busca_simples
            logger.info(f"[VER_DETALHES] IP {ip}: {len(specific_orders)} pedidos, {len(clientes_unicos)} clientes √∫nicos")
            
            # Se h√° apenas 1 cliente √∫nico, n√£o deveria estar na lista (erro de l√≥gica)
            if len(clientes_unicos) <= 1:
                logger.warning(f"IP {ip} tem apenas {len(clientes_unicos)} cliente(s) √∫nico(s) - n√£o deveria estar na lista")
                return Response({
                    'success': True,
                    'data': {
                        'ip': ip,
                        'total_orders': 0,
                        'client_details': [],
                        'active_orders': 0,
                        'cancelled_orders': 0
                    },
                    'loja_nome': config.nome_loja,
                    'message': f'IP {ip} n√£o tem clientes diferentes - dados inconsistentes'
                })
            
            # Usa TODOS os pedidos pois j√° passou no filtro da tabela principal
            pedidos_filtrados = specific_orders
            
            # Processa os dados para o formato esperado pelo frontend
            client_details = []
            active_orders = 0
            cancelled_orders = 0
            
            logger.info(f"Processando {len(pedidos_filtrados)} pedidos filtrados do grupo (clientes √∫nicos: {len(clientes_unicos)})")
            
            for order in pedidos_filtrados:
                try:
                    # Extrai dados do cliente
                    customer = order.get('customer', {})
                    customer_name = f"{customer.get('first_name', '')} {customer.get('last_name', '')}".strip()
                    if not customer_name:
                        customer_name = 'Cliente n√£o informado'
                    
                    # Busca detalhes de endere√ßo do pedido (PROTEGIDO COM TRY/CATCH)
                    shipping_city = ''
                    shipping_state = ''
                    
                    try:
                        order_details = detector.get_order_details(order['id'])
                        if order_details:
                            shipping_address = order_details.get('shipping_address', {})
                            shipping_city = shipping_address.get('city', '')
                            shipping_state = shipping_address.get('province', '')
                    except Exception as order_detail_error:
                        logger.warning(f"Erro ao buscar detalhes do pedido {order['id']}: {str(order_detail_error)}")
                        # Continua sem os detalhes de endere√ßo
                    
                    # Determina status do pedido (CORRIGIDO - usa cancelled_at)
                    is_cancelled = order.get('cancelled_at') is not None
                    status = 'cancelled' if is_cancelled else 'active'
                    
                    if is_cancelled:
                        cancelled_orders += 1
                    else:
                        active_orders += 1
                    
                    # Formata data de cria√ß√£o
                    created_at = order.get('created_at', '')
                    cancelled_at = order.get('cancelled_at')
                    
                    client_details.append({
                        'order_id': str(order['id']),
                        'order_number': order.get('order_number', ''),
                        'created_at': created_at,
                        'cancelled_at': cancelled_at,
                        'status': status,
                        'total_price': order.get('total_price', '0.00'),
                        'currency': order.get('currency', 'BRL'),
                        'customer_name': customer_name,
                        'customer_email': customer.get('email', ''),
                        'customer_phone': customer.get('phone', ''),
                        'shipping_city': shipping_city,
                        'shipping_state': shipping_state
                    })
                    
                except Exception as order_process_error:
                    logger.error(f"Erro ao processar pedido {order.get('id', 'unknown')}: {str(order_process_error)}")
                    # Continua processando outros pedidos
                    continue
            
            logger.info(f"Processamento conclu√≠do: {len(client_details)} pedidos processados, {active_orders} ativos, {cancelled_orders} cancelados")
            
            # Calcula estat√≠sticas dos pedidos filtrados
            total_sales = sum(float(order.get('total_price', 0)) for order in pedidos_filtrados)
            unique_customers = set()
            currencies = set()
            first_order_date = None
            last_order_date = None
            
            for order in pedidos_filtrados:
                customer = order.get('customer', {})
                if customer.get('email'):
                    unique_customers.add(customer['email'])
                elif customer.get('phone'):
                    unique_customers.add(customer['phone'])
                
                currency = order.get('currency', 'BRL')
                currencies.add(currency)
                
                order_date = order.get('created_at')
                if order_date:
                    if not first_order_date or order_date < first_order_date:
                        first_order_date = order_date
                    if not last_order_date or order_date > last_order_date:
                        last_order_date = order_date
            
            main_currency = list(currencies)[0] if currencies else 'BRL'
            ip_source = specific_orders[0].get('_ip_source', 'unknown') if specific_orders else 'unknown'
            
            # Monta resposta com dados reais
            response_data = {
                'success': True,
                'data': {
                    'ip': ip,
                    'total_orders': len(pedidos_filtrados),
                    'client_details': client_details,
                    'active_orders': active_orders,
                    'cancelled_orders': cancelled_orders,
                    'unique_customers': len(unique_customers),
                    'total_sales': f"{total_sales:.2f}",
                    'currency': main_currency,
                    'date_range': {
                        'first': first_order_date,
                        'last': last_order_date
                    },
                    'is_suspicious': False,  # Calculado posteriormente se necess√°rio
                    'ip_source': ip_source
                },
                'loja_nome': config.nome_loja,
                'period_days': days
            }
            
            # Log da busca bem-sucedida
            try:
                ProcessamentoLog.objects.create(
                    user=request.user,
                    config=config,
                    tipo='detalhamento_ip',
                    status='sucesso',
                    pedidos_encontrados=len(client_details),
                    detalhes={
                        'ip_consultado': ip,
                        'period_days': days,
                        'active_orders': active_orders,
                        'cancelled_orders': cancelled_orders
                    }
                )
                logger.info(f"Detalhamento IP conclu√≠do com sucesso - User: {getattr(request.user, 'username', 'Anonymous')}")
            except Exception as log_error:
                logger.error(f"Erro ao salvar log (n√£o cr√≠tico): {str(log_error)}")
            
            # === ARMAZENA NO CACHE PARA OTIMIZA√á√ÉO ===
            try:
                cache_manager.cache_ip_details(loja_id, ip, days, response_data['data'])
                logger.info(f"Dados do IP {ip} armazenados em cache para otimiza√ß√£o futura")
            except Exception as cache_error:
                logger.warning(f"Erro ao armazenar em cache (n√£o cr√≠tico): {str(cache_error)}")
            
            return Response(response_data)
            
        except HTTPError as http_error:
            # Log de erro HTTP espec√≠fico
            logger.error(f"Erro HTTP na busca por detalhes do IP: {str(http_error)}", exc_info=True)
            return Response({
                'error': 'Erro ao acessar dados do Shopify',
                'details': 'Verifique a configura√ß√£o da loja e token de acesso'
            }, status=status.HTTP_503_SERVICE_UNAVAILABLE)
            
        except (ConnectionError, Timeout) as conn_error:
            # Log de erro de conex√£o
            logger.error(f"Erro de conectividade na busca por detalhes do IP: {str(conn_error)}", exc_info=True)
            return Response({
                'error': 'Problema de conectividade com o Shopify',
                'details': 'Tente novamente em alguns instantes'
            }, status=status.HTTP_503_SERVICE_UNAVAILABLE)
            
        except Exception as shopify_error:
            # Log de erro gen√©rico do Shopify
            logger.error(f"Erro gen√©rico na busca do Shopify: {str(shopify_error)}", exc_info=True)
            return Response({
                'error': 'Erro ao processar dados do Shopify',
                'details': f'Erro espec√≠fico: {str(shopify_error)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
    except Exception as e:
        # Log de erro gen√©rico
        logger.error(f"ERRO CR√çTICO em detalhar_pedidos_ip - User: {getattr(request.user, 'username', 'Anonymous') if hasattr(request, 'user') else 'Unknown'}, Error: {str(e)}", exc_info=True)
        
        # Log do erro no banco
        try:
            if 'config' in locals():
                ProcessamentoLog.objects.create(
                    user=request.user,
                    config=config,
                    tipo='detalhamento_ip',
                    status='erro',
                    erro_mensagem=str(e),
                    detalhes={
                        'ip_consultado': ip if 'ip' in locals() else 'unknown',
                        'error_type': type(e).__name__
                    }
                )
        except Exception as log_error:
            logger.error(f"Erro ao salvar log de erro: {str(log_error)}")
        
        return Response({
            'error': 'Erro interno do servidor',
            'details': f'Erro espec√≠fico: {str(e)}'
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

@csrf_exempt
@api_view(['GET', 'POST'])
def test_simple_endpoint(request):
    """Endpoint de teste que agora busca IPs duplicados - vers√£o simples"""
    try:
        logger.info(f"Method recebido: {request.method}")
        logger.info(f"Data recebida: {request.data}")
        
        if request.method == 'GET':
            return Response({
                'success': True,
                'message': 'VERS√ÉO NOVA - Endpoint simples funcionando - GET',
                'timestamp': datetime.now().isoformat()
            })
        
        # POST = Busca IPs duplicados
        loja_id = request.data.get('loja_id')
        days = request.data.get('days', 30)
        
        if not loja_id:
            return Response({'error': 'ID da loja √© obrigat√≥rio'}, status=status.HTTP_400_BAD_REQUEST)
        
        config = ShopifyConfig.objects.filter(id=loja_id, ativo=True).first()
        if not config:
            return Response({'error': 'Loja n√£o encontrada'}, status=status.HTTP_400_BAD_REQUEST)
        
        # Inicializa conex√£o Shopify
        import shopify
        shop_url = config.shop_url
        if not shop_url.startswith('https://'):
            shop_url = f"https://{shop_url}"
        
        session = shopify.Session(shop_url, "2023-10", config.access_token)
        shopify.ShopifyResource.activate_session(session)
        
        # ‚ö° CORRE√á√ÉO CR√çTICA: Implementa pagina√ß√£o completa para buscar TODOS os pedidos
        data_inicial = timezone.now() - timedelta(days=days)
        
        # Busca TODOS os pedidos do per√≠odo usando pagina√ß√£o
        orders = []
        page_info = None
        page = 1
        
        logger.info(f"üîÑ Iniciando busca paginada (vers√£o simples) para {days} dias")
        
        while True:
            try:
                if page_info:
                    # P√°ginas subsequentes usam page_info
                    api_orders = shopify.Order.find(
                        limit=250,
                        page_info=page_info,
                        fields='id,name,created_at,browser_ip,customer'
                    )
                else:
                    # Primeira p√°gina usa filtros de data
                    api_orders = shopify.Order.find(
                        status='any',
                        created_at_min=data_inicial.isoformat(),
                        limit=250,
                        fields='id,name,created_at,browser_ip,customer'
                    )
                
                if not api_orders:
                    break
                
                orders.extend(api_orders)
                logger.info(f"üìÑ P√°gina {page}: {len(api_orders)} pedidos (Total: {len(orders)})")
                
                # Verifica pr√≥xima p√°gina
                try:
                    page_info = None
                    if hasattr(shopify.ShopifyResource, 'connection') and hasattr(shopify.ShopifyResource.connection, 'response'):
                        link_header = shopify.ShopifyResource.connection.response.headers.get('Link', '')
                        if link_header:
                            import re
                            match = re.search(r'<[^>]*page_info=([^&>]+)[^>]*>;\s*rel="next"', link_header)
                            if match:
                                page_info = match.group(1)
                            else:
                                break
                        else:
                            break
                    else:
                        break
                except Exception:
                    break
                
                page += 1
                if page > 50:  # Limite de seguran√ßa
                    break
                    
            except Exception as e:
                logger.error(f"‚ùå Erro na busca paginada p√°gina {page}: {e}")
                if page == 1:
                    raise e  # Falha na primeira p√°gina √© cr√≠tica
                else:
                    break  # P√°ginas posteriores podem falhar
        
        logger.info(f"‚úÖ Busca paginada conclu√≠da: {len(orders)} pedidos encontrados")
        
        # Agrupa pedidos por IP
        ip_groups = {}
        
        for order in orders:
            browser_ip = getattr(order, 'browser_ip', None)
            
            if browser_ip and browser_ip.strip():
                if browser_ip not in ip_groups:
                    ip_groups[browser_ip] = []
                
                ip_groups[browser_ip].append({
                    'id': order.id,
                    'number': order.name,
                    'created_at': order.created_at,
                    'customer_name': getattr(order.customer, 'first_name', '') + ' ' + getattr(order.customer, 'last_name', '') if order.customer else 'N/A'
                })
        
        # ‚ö° CORRE√á√ÉO: Filtra TODOS os IPs com 2+ pedidos (independente do cliente)
        ips_duplicados = []
        for ip, pedidos in ip_groups.items():
            if len(pedidos) >= 2:  # QUALQUER IP com 2+ pedidos
                # Ordena por data
                pedidos_ordenados = sorted(pedidos, key=lambda x: x['created_at'])
                
                # Conta clientes √∫nicos para an√°lise
                clientes_unicos = set()
                for pedido in pedidos:
                    cliente = pedido.get('customer_name', 'N/A')
                    if cliente and cliente != 'N/A':
                        clientes_unicos.add(cliente)
                
                ips_duplicados.append({
                    'browser_ip': ip,
                    'total_pedidos': len(pedidos),
                    'clientes_unicos': len(clientes_unicos),
                    'clientes_diferentes': len(clientes_unicos) > 1,  # Para an√°lise no frontend
                    'pedidos': pedidos_ordenados,
                    'primeiro_pedido': pedidos_ordenados[0]['created_at'],
                    'ultimo_pedido': pedidos_ordenados[-1]['created_at']
                })
        
        # Ordena por quantidade de pedidos (mais pedidos primeiro)
        ips_duplicados.sort(key=lambda x: x['total_pedidos'], reverse=True)
        
        return Response({
            'success': True,
            'ips_duplicados': ips_duplicados,
            'total_ips': len(ips_duplicados),
            'total_pedidos': sum(ip['total_pedidos'] for ip in ips_duplicados),
            'days_searched': days,
            'loja_nome': config.nome_loja,
            'message': f'Busca de IPs realizada com sucesso! {len(ips_duplicados)} IPs com m√∫ltiplos pedidos encontrados.'
        })
        
    except Exception as e:
        return Response({'error': f'Erro: {str(e)}'}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

@csrf_exempt
@api_view(['POST'])
def debug_detector_ip(request):
    """Endpoint para debugar problemas no detector de IP"""
    try:
        loja_id = request.data.get('loja_id')
        if not loja_id:
            return Response({'error': 'ID da loja √© obrigat√≥rio'}, status=status.HTTP_400_BAD_REQUEST)
        
        logger.info(f"DEBUG: Iniciando debug do detector para loja {loja_id}")
        
        # Busca configura√ß√£o
        try:
            config = ShopifyConfig.objects.filter(id=loja_id, ativo=True).first()
            if not config:
                return Response({'error': 'Loja n√£o encontrada'}, status=status.HTTP_404_NOT_FOUND)
            
            logger.info(f"DEBUG: Loja encontrada - {config.nome_loja}")
        except Exception as config_error:
            logger.error(f"DEBUG: Erro ao buscar configura√ß√£o: {str(config_error)}")
            return Response({'error': f'Erro de configura√ß√£o: {str(config_error)}'}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        
        # Testa detector
        try:
            detector = ShopifyDuplicateOrderDetector(config.shop_url, config.access_token)
            logger.info(f"DEBUG: Detector criado com sucesso")
            
            # Testa conex√£o
            connection_ok, test_message = detector.test_connection()
            logger.info(f"DEBUG: Teste de conex√£o - OK: {connection_ok}, Message: {test_message}")
            
            if not connection_ok:
                return Response({
                    'error': 'Falha na conex√£o',
                    'details': test_message
                }, status=status.HTTP_401_UNAUTHORIZED)
            
        except Exception as detector_error:
            logger.error(f"DEBUG: Erro ao criar/testar detector: {str(detector_error)}", exc_info=True)
            return Response({'error': f'Erro no detector: {str(detector_error)}'}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        
        # Testa busca de IPs (vers√£o minimalista)
        try:
            logger.info(f"DEBUG: Iniciando busca por IPs (1 dia, min 1 pedido)")
            ip_data = detector.get_orders_by_ip(days=1, min_orders=1)
            logger.info(f"DEBUG: Busca conclu√≠da - Total IPs: {ip_data.get('total_ips_found', 0)}")
            
            return Response({
                'success': True,
                'message': 'Debug conclu√≠do com sucesso',
                'loja_nome': config.nome_loja,
                'connection_status': 'OK',
                'ip_search_result': {
                    'total_ips_found': ip_data.get('total_ips_found', 0),
                    'total_orders': ip_data.get('total_orders_analyzed', 0),
                    'debug_stats': ip_data.get('debug_stats', {})
                },
                'timestamp': timezone.now().isoformat()
            })
            
        except Exception as search_error:
            logger.error(f"DEBUG: Erro na busca por IPs: {str(search_error)}", exc_info=True)
            return Response({
                'error': f'Erro na busca: {str(search_error)}',
                'loja_nome': config.nome_loja,
                'connection_status': 'OK',
                'search_status': 'FAILED'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        
    except Exception as e:
        logger.error(f"DEBUG: Erro geral no debug: {str(e)}", exc_info=True)
        return Response({'error': f'Erro geral: {str(e)}'}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

@csrf_exempt
@api_view(['POST'])
@never_cache
def test_detalhar_ip(request):
    """Endpoint de teste simplificado para diagnosticar o erro 500"""
    try:
        loja_id = request.data.get('loja_id')
        ip = request.data.get('ip')
        
        return Response({
            'success': True,
            'message': 'Endpoint de teste funcionando',
            'data': {
                'ip': ip,
                'total_orders': 2,
                'active_orders': 1,
                'cancelled_orders': 1,
                'client_details': [
                    {
                        'order_id': 'test-123',
                        'order_number': 'TEST123',
                        'created_at': '2024-08-15T10:00:00Z',
                        'cancelled_at': None,
                        'status': 'active',
                        'total_price': '49.90',
                        'currency': 'BRL',
                        'customer_name': 'Cliente Teste',
                        'customer_email': 'teste@exemplo.com',
                        'customer_phone': None,
                        'shipping_city': 'S√£o Paulo',
                        'shipping_state': 'SP'
                    }
                ]
            }
        })
    except Exception as e:
        logger.error(f"Erro no teste detalhar IP: {str(e)}")
        return Response({'error': f'Erro: {str(e)}'}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

@csrf_exempt
@api_view(['GET'])
def status_lojas_shopify(request):
    """Retorna status de conectividade de todas as lojas Shopify configuradas"""
    try:
        configs = ShopifyConfig.objects.filter(ativo=True)
        
        if not configs.exists():
            return Response({
                'success': False,
                'message': 'Nenhuma loja Shopify configurada',
                'lojas_status': [],
                'action_required': 'add_shopify_store',
                'setup_guide': {
                    'title': 'Como configurar uma loja Shopify',
                    'steps': [
                        '1. Acesse o Admin da sua loja Shopify',
                        '2. V√° em Settings > Apps and sales channels',
                        '3. Clique em "Develop apps"',
                        '4. Crie um novo app privado',
                        '5. Configure permiss√µes: read_orders, write_orders, read_customers',
                        '6. Instale o app e copie o Access Token',
                        '7. Use POST /api/processamento/lojas-config/ para adicionar'
                    ]
                }
            })
        
        lojas_status = []
        total_funcionais = 0
        
        for config in configs:
            detector = ShopifyDuplicateOrderDetector(config.shop_url, config.access_token)
            
            try:
                connection_ok, message = detector.test_connection()
                
                status_info = {
                    'id': config.id,
                    'nome_loja': config.nome_loja,
                    'shop_url': config.shop_url,
                    'data_criacao': config.data_criacao.isoformat(),
                    'conectado': connection_ok,
                    'message': message,
                    'status': 'funcional' if connection_ok else 'erro',
                    'token_length': len(config.access_token) if config.access_token else 0,
                    'api_version': config.api_version
                }
                
                if connection_ok:
                    total_funcionais += 1
                
                lojas_status.append(status_info)
                
            except Exception as e:
                lojas_status.append({
                    'id': config.id,
                    'nome_loja': config.nome_loja,
                    'shop_url': config.shop_url,
                    'data_criacao': config.data_criacao.isoformat(),
                    'conectado': False,
                    'message': f'Erro de conex√£o: {str(e)}',
                    'status': 'erro_critico',
                    'token_length': len(config.access_token) if config.access_token else 0,
                    'api_version': config.api_version
                })
        
        # Determina status geral
        if total_funcionais == 0:
            status_geral = 'todas_com_erro'
            message_geral = 'Todas as lojas apresentam problemas de conectividade'
        elif total_funcionais == len(lojas_status):
            status_geral = 'todas_funcionais'
            message_geral = 'Todas as lojas est√£o funcionando corretamente'
        else:
            status_geral = 'parcialmente_funcional'
            message_geral = f'{total_funcionais} de {len(lojas_status)} lojas funcionais'
        
        return Response({
            'success': True,
            'status_geral': status_geral,
            'message': message_geral,
            'total_lojas': len(lojas_status),
            'lojas_funcionais': total_funcionais,
            'lojas_com_erro': len(lojas_status) - total_funcionais,
            'lojas_status': lojas_status,
            'timestamp': timezone.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Erro ao verificar status das lojas Shopify: {str(e)}")
        return Response({
            'error': 'Erro interno ao verificar status das lojas'
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

@csrf_exempt
@api_view(['GET'])
def historico_logs(request):
    """Retorna hist√≥rico de opera√ß√µes (opcionalmente filtrado por loja)"""
    try:
        loja_id = request.GET.get('loja_id')
        
        if loja_id:
            logs = ProcessamentoLog.objects.filter(
                user=request.user, 
                config_id=loja_id
            ).order_by('-data_execucao')[:50]
        else:
            logs = ProcessamentoLog.objects.filter(user=request.user).order_by('-data_execucao')[:50]
        
        logs_data = []
        for log in logs:
            logs_data.append({
                'id': log.id,
                'loja_nome': log.config.nome_loja,
                'tipo': log.get_tipo_display(),
                'status': log.get_status_display(),
                'pedidos_encontrados': log.pedidos_encontrados,
                'pedidos_cancelados': log.pedidos_cancelados,
                'data_execucao': log.data_execucao.isoformat(),
                'erro_mensagem': log.erro_mensagem,
                'detalhes': log.detalhes
            })
        
        return Response({'logs': logs_data})
        
    except Exception as e:
        return Response({'error': str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

@csrf_exempt
@api_view(['POST'])

# === FUN√á√ïES AUXILIARES ===

def _sanitize_order_details(order_details):
    """
    Remove dados muito sens√≠veis dos detalhes do pedido
    
    Args:
        order_details: Detalhes completos do pedido
        
    Returns:
        dict: Detalhes sanitizados
    """
    if not order_details:
        return order_details
    
    sanitized = order_details.copy()
    
    # Remove coordenadas exatas (muito espec√≠ficas para localiza√ß√£o)
    for address_type in ['shipping_address', 'billing_address']:
        if address_type in sanitized and sanitized[address_type]:
            address = sanitized[address_type]
            # Mant√©m dados gerais mas remove coordenadas exatas
            address.pop('latitude', None)
            address.pop('longitude', None)
            
            # Mascara parte do CEP para reduzir precis√£o da localiza√ß√£o
            if 'zip' in address and address['zip']:
                zip_code = str(address['zip'])
                if len(zip_code) >= 5:
                    address['zip'] = zip_code[:3] + 'xxx'
    
    # Remove informa√ß√µes muito espec√≠ficas do cliente
    if 'customer_info' in sanitized:
        customer = sanitized['customer_info']
        # Remove notas privadas que podem conter informa√ß√µes sens√≠veis
        customer.pop('note', None)
        customer.pop('multipass_identifier', None)
    
    return sanitized


def _analyze_ip_fields(raw_order):
    """
    Analisa todos os campos relacionados a IP no pedido RAW do Shopify
    
    Args:
        raw_order: Dados brutos do pedido do Shopify
        
    Returns:
        dict: An√°lise completa dos campos de IP encontrados
    """
    ip_analysis = {
        'ips_found': [],
        'ip_fields_structure': {},
        'potential_ip_sources': [],
        'client_details_analysis': {},
        'address_analysis': {},
        'customer_analysis': {},
        'order_level_analysis': {}
    }
    
    # Fun√ß√£o auxiliar para verificar se um valor parece ser IP (CORRIGIDA)
    def looks_like_ip(value):
        if not value or not isinstance(value, str):
            return False
        value = value.strip()
        
        # Valida√ß√£o IPv4
        if '.' in value:
            parts = value.split('.')
            if len(parts) == 4:
                try:
                    for part in parts:
                        num = int(part)
                        if not (0 <= num <= 255):
                            return False
                    return True
                except ValueError:
                    return False
        
        # Valida√ß√£o IPv6 (simplificada)
        elif ':' in value and len(value) > 7:
            # IPv6 deve ter pelo menos 2 grupos separados por :
            return len(value.split(':')) >= 2
        
        return False
    
    # Fun√ß√£o auxiliar para analisar objeto recursivamente
    def analyze_object(obj, path=""):
        for key, value in obj.items():
            current_path = f"{path}.{key}" if path else key
            
            if isinstance(value, dict):
                # Recurs√£o em objetos
                analyze_object(value, current_path)
            elif looks_like_ip(value):
                # Encontrou poss√≠vel IP
                ip_analysis['ips_found'].append({
                    'ip': value,
                    'source_path': current_path,
                    'source_description': current_path
                })
                ip_analysis['potential_ip_sources'].append(current_path)
            elif 'ip' in key.lower():
                # Campo relacionado a IP (mesmo que vazio)
                ip_analysis['ip_fields_structure'][current_path] = {
                    'value': value,
                    'type': type(value).__name__,
                    'is_ip_like': looks_like_ip(value)
                }
    
    # 1. AN√ÅLISE DO CLIENT_DETAILS (principal fonte atual)
    client_details = raw_order.get('client_details', {})
    if client_details:
        ip_analysis['client_details_analysis'] = {
            'exists': True,
            'all_fields': list(client_details.keys()),
            'ip_related_fields': {}
        }
        
        for key, value in client_details.items():
            if 'ip' in key.lower() or looks_like_ip(value):
                ip_analysis['client_details_analysis']['ip_related_fields'][key] = {
                    'value': value,
                    'looks_like_ip': looks_like_ip(value)
                }
        
        analyze_object(client_details, 'client_details')
    else:
        ip_analysis['client_details_analysis'] = {'exists': False}
    
    # 2. AN√ÅLISE DOS ENDERE√áOS
    for address_type in ['shipping_address', 'billing_address']:
        address = raw_order.get(address_type, {})
        if address:
            address_analysis = {
                'exists': True,
                'all_fields': list(address.keys()),
                'ip_related_fields': {}
            }
            
            for key, value in address.items():
                if 'ip' in key.lower() or looks_like_ip(value):
                    address_analysis['ip_related_fields'][key] = {
                        'value': value,
                        'looks_like_ip': looks_like_ip(value)
                    }
            
            ip_analysis['address_analysis'][address_type] = address_analysis
            analyze_object(address, address_type)
        else:
            ip_analysis['address_analysis'][address_type] = {'exists': False}
    
    # 3. AN√ÅLISE DO CUSTOMER
    customer = raw_order.get('customer', {})
    if customer:
        ip_analysis['customer_analysis'] = {
            'exists': True,
            'all_fields': list(customer.keys()),
            'ip_related_fields': {}
        }
        
        # Analisa customer principal
        for key, value in customer.items():
            if 'ip' in key.lower() or looks_like_ip(value):
                ip_analysis['customer_analysis']['ip_related_fields'][key] = {
                    'value': value,
                    'looks_like_ip': looks_like_ip(value)
                }
        
        # Analisa default_address do customer (fonte priorit√°ria na nova l√≥gica)
        default_address = customer.get('default_address', {})
        if default_address:
            ip_analysis['customer_analysis']['default_address'] = {
                'exists': True,
                'all_fields': list(default_address.keys()),
                'ip_related_fields': {}
            }
            
            for key, value in default_address.items():
                if 'ip' in key.lower() or looks_like_ip(value):
                    ip_analysis['customer_analysis']['default_address']['ip_related_fields'][key] = {
                        'value': value,
                        'looks_like_ip': looks_like_ip(value)
                    }
            
            analyze_object(default_address, 'customer.default_address')
        else:
            ip_analysis['customer_analysis']['default_address'] = {'exists': False}
        
        analyze_object(customer, 'customer')
    else:
        ip_analysis['customer_analysis'] = {'exists': False}
    
    # 4. AN√ÅLISE DO N√çVEL DO PEDIDO
    order_ip_fields = {}
    for key, value in raw_order.items():
        if 'ip' in key.lower() or looks_like_ip(value):
            order_ip_fields[key] = {
                'value': value,
                'looks_like_ip': looks_like_ip(value)
            }
    
    ip_analysis['order_level_analysis'] = {
        'ip_related_fields': order_ip_fields,
        'total_root_fields': len(raw_order.keys())
    }
    
    # 5. RESUMO FINAL
    ip_analysis['summary'] = {
        'total_ips_found': len(ip_analysis['ips_found']),
        'total_ip_fields': len(ip_analysis['ip_fields_structure']),
        'has_client_details': bool(client_details),
        'has_shipping_address': bool(raw_order.get('shipping_address')),
        'has_billing_address': bool(raw_order.get('billing_address')),
        'has_customer_data': bool(customer),
        'has_customer_default_address': bool(customer.get('default_address')) if customer else False
    }
    
    return ip_analysis

def _analyze_ip_fields_improved(raw_order):
    """
    An√°lise MELHORADA de IPs usando a l√≥gica do detector principal
    Remove falsos positivos e usa hierarquia correta de busca
    """
    from .services.shopify_detector import ShopifyDuplicateOrderDetector
    
    # Cria inst√¢ncia tempor√°ria apenas para usar m√©todos de valida√ß√£o
    detector = ShopifyDuplicateOrderDetector("temp.myshopify.com", "temp_token")
    
    # Usa a fun√ß√£o real do detector para encontrar o IP principal
    main_ip, ip_source = detector._extract_real_customer_ip(raw_order)
    
    # Verifica se IP √© suspeito
    is_suspicious = False
    suspicious_pattern = None
    if main_ip:
        is_suspicious = detector._is_suspicious_ip(main_ip)
        if is_suspicious:
            suspicious_pattern = detector._get_suspicious_pattern(main_ip)
    
    # An√°lise estrutural detalhada
    analysis = {
        'primary_ip_found': bool(main_ip),
        'primary_ip': main_ip,
        'primary_ip_source': ip_source,
        'is_suspicious': is_suspicious,
        'suspicious_pattern': suspicious_pattern,
        'structure_analysis': {}
    }
    
    # An√°lise detalhada da estrutura dispon√≠vel
    sections = {
        'client_details': raw_order.get('client_details', {}),
        'customer_default_address': raw_order.get('customer', {}).get('default_address', {}),
        'shipping_address': raw_order.get('shipping_address', {}),
        'billing_address': raw_order.get('billing_address', {}),
        'customer_root': raw_order.get('customer', {}),
        'order_root': raw_order
    }
    
    for section_name, section_data in sections.items():
        if not section_data:
            analysis['structure_analysis'][section_name] = {'exists': False}
            continue
            
        # Encontra campos relacionados a IP
        ip_fields = {}
        all_fields = []
        
        for key, value in section_data.items():
            all_fields.append(key)
            
            # Verifica se √© campo relacionado a IP
            if 'ip' in key.lower() and isinstance(value, str) and value.strip():
                ip_fields[key] = {
                    'value': value,
                    'is_valid_ip': detector._is_valid_ip(value),
                    'is_suspicious': detector._is_suspicious_ip(value) if detector._is_valid_ip(value) else False
                }
        
        analysis['structure_analysis'][section_name] = {
            'exists': True,
            'total_fields': len(all_fields),
            'all_fields': all_fields,
            'ip_related_fields': ip_fields,
            'has_valid_ip': bool([f for f in ip_fields.values() if f['is_valid_ip']])
        }
    
    # Resumo executivo
    analysis['summary'] = {
        'ip_detection_successful': bool(main_ip),
        'ip_source_hierarchy_position': _get_hierarchy_position(ip_source) if ip_source else None,
        'sections_with_ips': len([s for s in analysis['structure_analysis'].values() if s.get('has_valid_ip', False)]),
        'total_sections_analyzed': len([s for s in analysis['structure_analysis'].values() if s.get('exists', False)]),
        'detection_method': 'shopify_detector_logic'
    }
    
    # Recomenda√ß√µes
    analysis['recommendations'] = []
    if not main_ip:
        analysis['recommendations'].append("PROBLEMA: Nenhum IP v√°lido encontrado - verificar estrutura de dados")
        if analysis['structure_analysis']['client_details']['exists']:
            analysis['recommendations'].append("client_details existe mas n√£o tem browser_ip v√°lido")
    else:
        if is_suspicious:
            analysis['recommendations'].append(f"ATEN√á√ÉO: IP detectado como suspeito ({suspicious_pattern})")
        analysis['recommendations'].append(f"SUCESSO: IP encontrado via {ip_source}")
    
    return analysis

def _get_hierarchy_position(ip_source):
    """Retorna posi√ß√£o na hierarquia de busca baseado na fonte do IP"""
    if not ip_source:
        return None
        
    hierarchy_map = {
        'client_details.browser_ip': 1,
        'customer.default_address': 2,
        'shipping_address': 3,
        'billing_address': 4,
        'customer': 5,
        'client_details': 6,
        'order': 7
    }
    
    # Encontra posi√ß√£o baseado no prefixo da fonte
    for prefix, position in hierarchy_map.items():
        if ip_source.startswith(prefix):
            return position
    
    return 99  # Desconhecido


def _extract_ip_field_paths(raw_order):
    """
    Extrai todos os caminhos de campos que cont√™m ou podem conter IPs no pedido RAW
    Inclui campos mesmo se estiverem vazios - √∫til para debug
    
    Args:
        raw_order: Dados completos do pedido
        
    Returns:
        list: Lista completa de caminhos relacionados a IP encontrados
    """
    ip_paths = []
    all_ip_related_paths = []
    
    def looks_like_ip(value):
        """Verifica se um valor parece ser um IP (IPv4 ou IPv6)"""
        if not value:
            return False
        
        value_str = str(value).strip()
        if not value_str:
            return False
            
        # IPv4 - mais rigoroso
        if '.' in value_str and len(value_str.split('.')) == 4:
            try:
                parts = value_str.split('.')
                # Verifica se todas as partes s√£o n√∫meros v√°lidos de 0-255
                if all(part.isdigit() and 0 <= int(part) <= 255 for part in parts):
                    return True
            except:
                pass
        
        # IPv6 - b√°sico
        if ':' in value_str and 7 <= len(value_str) <= 39:
            # Verifica padr√£o b√°sico de IPv6
            if value_str.count(':') >= 2:
                return True
        
        return False
    
    def is_ip_related_field(field_name):
        """Verifica se o nome do campo indica que pode conter IP"""
        field_lower = field_name.lower()
        ip_indicators = [
            'ip', 'browser_ip', 'client_ip', 'customer_ip', 'remote_ip',
            'forwarded_ip', 'real_ip', 'origin_ip', 'proxy_ip', 'x_forwarded',
            'x_real_ip', 'cf_connecting_ip', 'true_client_ip'
        ]
        
        return any(indicator in field_lower for indicator in ip_indicators)
    
    def scan_object(obj, path=""):
        """Escaneia recursivamente um objeto procurando por IPs e campos relacionados"""
        if isinstance(obj, dict):
            for key, value in obj.items():
                current_path = f"{path}.{key}" if path else key
                
                # Verifica se √© campo relacionado a IP (mesmo que vazio)
                if is_ip_related_field(key):
                    all_ip_related_paths.append({
                        'path': current_path,
                        'field_name': key,
                        'value': value,
                        'has_ip_value': looks_like_ip(value),
                        'is_empty': value is None or value == '',
                        'type': 'ip_field_name'
                    })
                
                # Verifica se o valor parece ser IP
                if looks_like_ip(value):
                    ip_paths.append({
                        'path': current_path,
                        'field_name': key,
                        'ip_value': value,
                        'type': 'ip_value_found'
                    })
                
                # Recurs√£o para objetos aninhados
                if isinstance(value, (dict, list)):
                    scan_object(value, current_path)
                    
        elif isinstance(obj, list):
            for i, item in enumerate(obj):
                current_path = f"{path}[{i}]"
                scan_object(item, current_path)
    
    # === AN√ÅLISE ESPECIAL DE CAMPOS CONHECIDOS ===
    
    # 1. client_details - fonte principal
    if 'client_details' in raw_order:
        client_details = raw_order['client_details']
        if isinstance(client_details, dict):
            all_ip_related_paths.append({
                'path': 'client_details',
                'field_name': 'client_details',
                'value': client_details,
                'has_ip_value': False,
                'is_empty': not bool(client_details),
                'type': 'ip_container_object',
                'all_fields': list(client_details.keys()) if client_details else []
            })
    
    # 2. Campos diretos no pedido
    direct_ip_candidates = [
        'browser_ip', 'client_ip', 'customer_ip', 'remote_ip', 'ip_address'
    ]
    
    for field in direct_ip_candidates:
        if field in raw_order:
            all_ip_related_paths.append({
                'path': field,
                'field_name': field,
                'value': raw_order[field],
                'has_ip_value': looks_like_ip(raw_order[field]),
                'is_empty': raw_order[field] is None or raw_order[field] == '',
                'type': 'direct_ip_field'
            })
    
    # 3. An√°lise completa recursiva
    scan_object(raw_order)
    
    # === RESULTADO COMPLETO ===
    result = {
        'ip_values_found': ip_paths,
        'ip_related_fields': all_ip_related_paths,
        'summary': {
            'total_ip_values': len(ip_paths),
            'total_ip_fields': len(all_ip_related_paths),
            'has_client_details': 'client_details' in raw_order,
            'client_details_fields': list(raw_order.get('client_details', {}).keys()) if raw_order.get('client_details') else []
        }
    }
    
    return result

# ===== NOVOS ENDPOINTS PARA M√âTODOS ALTERNATIVOS DE CAPTURA DE IP =====

@csrf_exempt
@api_view(['POST'])
@never_cache
def analyze_order_ip_alternative(request):
    """
    Analisa pedido usando m√©todos alternativos de captura de IP
    
    Usa os novos servi√ßos implementados para casos onde Shopify n√£o fornece IP
    """
    try:
        # Valida√ß√µes b√°sicas
        loja_id = request.data.get('loja_id')
        order_id = request.data.get('order_id')
        use_external_apis = request.data.get('use_external_apis', False)
        
        if not loja_id or not order_id:
            return Response({
                'error': 'ID da loja e ID do pedido s√£o obrigat√≥rios'
            }, status=status.HTTP_400_BAD_REQUEST)
        
        # Verifica loja
        config = ShopifyConfig.objects.filter(id=loja_id, ativo=True).first()
        if not config:
            return Response({
                'error': 'Loja n√£o encontrada ou inativa'
            }, status=status.HTTP_400_BAD_REQUEST)
        
        # Rate limiting
        allowed, remaining = RateLimitManager.check_rate_limit(request.user, 'ip_search')
        if not allowed:
            return RateLimitManager.get_rate_limit_response('ip_search')
        
        # Busca dados do pedido
        detector = ShopifyDuplicateOrderDetector(config.shop_url, config.access_token)
        order_data = detector.get_order_details(order_id)
        
        if not order_data:
            return Response({
                'error': 'Pedido n√£o encontrado'
            }, status=status.HTTP_404_NOT_FOUND)
        
        # Busca pedidos similares para an√°lise comportamental
        similar_orders = None
        try:
            # Busca outros pedidos do mesmo cliente
            customer_email = order_data.get('customer_info', {}).get('email')
            if customer_email:
                all_orders = detector.get_all_orders(days_back=30)
                similar_orders = [
                    order for order in all_orders 
                    if order.get('customer', {}).get('email') == customer_email
                    and order.get('id') != order_id
                ]
        except Exception as e:
            logger.warning(f"Erro ao buscar pedidos similares: {e}")
        
        # Usa detector melhorado
        improved_detector = get_improved_ip_detector()
        detection_result = improved_detector.detect_customer_ip(
            order_data, 
            similar_orders=similar_orders,
            use_external_apis=use_external_apis
        )
        
        # Auditoria
        audit_details = {
            'order_id': order_id,
            'loja_id': loja_id,
            'use_external_apis': use_external_apis,
            'method_used': detection_result.get('method_used'),
            'ip_found': bool(detection_result.get('customer_ip')),
            'confidence': detection_result.get('confidence_score', 0)
        }
        
        AuditLogger.log_ip_access(
            request.user,
            request,
            'alternative_ip_analysis',
            audit_details
        )
        
        # Log da opera√ß√£o
        ProcessamentoLog.objects.create(
            user=request.user,
            config=config,
            tipo='busca_ip',
            status='sucesso',
            pedidos_encontrados=1,
            detalhes=audit_details
        )
        
        response = Response({
            'success': True,
            'order_id': order_id,
            'loja_nome': config.nome_loja,
            'detection_result': detection_result,
            'analysis_timestamp': datetime.now().isoformat()
        })
        
        return SecurityHeadersManager.add_security_headers(response)
        
    except Exception as e:
        logger.error(f"Erro na an√°lise alternativa de IP: {e}")
        
        # Log do erro
        if 'config' in locals():
            ProcessamentoLog.objects.create(
                user=request.user,
                config=config,
                tipo='busca_ip',
                status='erro',
                erro_mensagem=str(e),
                detalhes={'order_id': order_id if 'order_id' in locals() else 'unknown'}
            )
        
        return Response({
            'error': 'Erro interno do servidor'
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

@csrf_exempt
@api_view(['POST'])
@never_cache
def batch_analyze_ips(request):
    """
    Analisa m√∫ltiplos pedidos em lote para detectar IPs
    """
    try:
        # Valida√ß√µes
        loja_id = request.data.get('loja_id')
        order_ids = request.data.get('order_ids', [])
        use_external_apis = request.data.get('use_external_apis', False)
        
        if not loja_id or not order_ids:
            return Response({
                'error': 'ID da loja e lista de pedidos s√£o obrigat√≥rios'
            }, status=status.HTTP_400_BAD_REQUEST)
        
        # Limita quantidade para evitar sobrecarga
        if len(order_ids) > 20:
            return Response({
                'error': 'M√°ximo de 20 pedidos por lote'
            }, status=status.HTTP_400_BAD_REQUEST)
        
        # Verifica loja
        config = ShopifyConfig.objects.filter(id=loja_id, ativo=True).first()
        if not config:
            return Response({
                'error': 'Loja n√£o encontrada ou inativa'
            }, status=status.HTTP_400_BAD_REQUEST)
        
        # Rate limiting mais rigoroso para lotes
        allowed, remaining = RateLimitManager.check_rate_limit(request.user, 'ip_search')
        if not allowed:
            return RateLimitManager.get_rate_limit_response('ip_search')
        
        # Busca dados dos pedidos
        detector = ShopifyDuplicateOrderDetector(config.shop_url, config.access_token)
        orders_data = []
        
        for order_id in order_ids:
            order_data = detector.get_order_details(order_id)
            if order_data:
                orders_data.append(order_data)
        
        if not orders_data:
            return Response({
                'error': 'Nenhum pedido v√°lido encontrado'
            }, status=status.HTTP_404_NOT_FOUND)
        
        # Processa em lote
        improved_detector = get_improved_ip_detector()
        batch_results = improved_detector.batch_detect_ips(
            orders_data,
            use_external_apis=use_external_apis,
            max_concurrent=5
        )
        
        # Auditoria
        audit_details = {
            'loja_id': loja_id,
            'total_orders': len(order_ids),
            'successful_detections': batch_results.get('successful_detections', 0),
            'use_external_apis': use_external_apis,
            'processing_time': batch_results.get('processing_time', 0)
        }
        
        AuditLogger.log_ip_access(
            request.user,
            request,
            'batch_ip_analysis',
            audit_details
        )
        
        # Log da opera√ß√£o
        ProcessamentoLog.objects.create(
            user=request.user,
            config=config,
            tipo='busca_ip',
            status='sucesso',
            pedidos_encontrados=len(orders_data),
            detalhes=audit_details
        )
        
        response = Response({
            'success': True,
            'loja_nome': config.nome_loja,
            'batch_results': batch_results,
            'analysis_timestamp': datetime.now().isoformat()
        })
        
        return SecurityHeadersManager.add_security_headers(response)
        
    except Exception as e:
        logger.error(f"Erro na an√°lise em lote de IPs: {e}")
        return Response({
            'error': 'Erro interno do servidor'
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

@csrf_exempt
@api_view(['POST'])
@never_cache
def analyze_ip_quality(request):
    """
    Analisa qualidade e confiabilidade de um IP espec√≠fico
    """
    try:
        # Valida√ß√µes
        ip_address = request.data.get('ip_address')
        order_id = request.data.get('order_id')  # Opcional
        loja_id = request.data.get('loja_id')    # Opcional
        
        if not ip_address:
            return Response({
                'error': 'Endere√ßo IP √© obrigat√≥rio'
            }, status=status.HTTP_400_BAD_REQUEST)
        
        # Sanitiza IP
        ip_sanitized = IPSecurityUtils.sanitize_ip_input(ip_address)
        if not ip_sanitized:
            return Response({
                'error': 'Formato de IP inv√°lido'
            }, status=status.HTTP_400_BAD_REQUEST)
        
        # Rate limiting
        allowed, remaining = RateLimitManager.check_rate_limit(request.user, 'ip_detail')
        if not allowed:
            return RateLimitManager.get_rate_limit_response('ip_detail')
        
        # Busca dados do pedido se fornecido
        order_data = None
        config = None
        
        if order_id and loja_id:
            config = ShopifyConfig.objects.filter(id=loja_id, ativo=True).first()
            if config:
                detector = ShopifyDuplicateOrderDetector(config.shop_url, config.access_token)
                order_data = detector.get_order_details(order_id)
        
        # Analisa qualidade do IP
        improved_detector = get_improved_ip_detector()
        quality_analysis = improved_detector.analyze_ip_quality(ip_sanitized, order_data)
        
        # Auditoria
        audit_details = {
            'ip_analyzed': IPSecurityUtils.mask_ip(ip_sanitized),
            'order_id': order_id,
            'loja_id': loja_id,
            'quality_score': quality_analysis.get('quality_score', 0),
            'recommendation': quality_analysis.get('recommendation')
        }
        
        AuditLogger.log_ip_access(
            request.user,
            request,
            'ip_quality_analysis',
            audit_details
        )
        
        response = Response({
            'success': True,
            'ip_analysis': quality_analysis,
            'analysis_timestamp': datetime.now().isoformat()
        })
        
        return SecurityHeadersManager.add_security_headers(response)
        
    except Exception as e:
        logger.error(f"Erro na an√°lise de qualidade do IP: {e}")
        return Response({
            'error': 'Erro interno do servidor'
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

@csrf_exempt
@api_view(['GET'])
def get_geolocation_status(request):
    """
    Retorna status dos servi√ßos de geolocaliza√ß√£o configurados
    """
    try:
        # Verifica configura√ß√µes
        config_validation = GeolocationConfig.validate_configuration()
        
        # Testa servi√ßos
        geo_service = get_geolocation_service()
        service_validation = geo_service.validate_services()
        service_status = geo_service.get_service_status()
        
        # Estat√≠sticas do detector melhorado
        improved_detector = get_improved_ip_detector()
        detection_stats = improved_detector.get_detection_statistics()
        
        response = Response({
            'success': True,
            'configuration': config_validation,
            'services_validation': service_validation,
            'services_status': service_status,
            'detection_statistics': detection_stats,
            'timestamp': datetime.now().isoformat()
        })
        
        return SecurityHeadersManager.add_security_headers(response)
        
    except Exception as e:
        logger.error(f"Erro ao obter status de geolocaliza√ß√£o: {e}")
        return Response({
            'error': 'Erro interno do servidor'
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

@csrf_exempt
@api_view(['POST'])
@never_cache
def test_geolocation_api(request):
    """
    Testa APIs de geolocaliza√ß√£o com IP de exemplo
    """
    try:
        # IP de teste (Google DNS)
        test_ip = request.data.get('test_ip', '8.8.8.8')
        
        # Sanitiza IP
        ip_sanitized = IPSecurityUtils.sanitize_ip_input(test_ip)
        if not ip_sanitized:
            return Response({
                'error': 'Formato de IP inv√°lido'
            }, status=status.HTTP_400_BAD_REQUEST)
        
        # Rate limiting
        allowed, remaining = RateLimitManager.check_rate_limit(request.user, 'ip_detail')
        if not allowed:
            return RateLimitManager.get_rate_limit_response('ip_detail')
        
        # Testa servi√ßo de geolocaliza√ß√£o
        geo_service = get_geolocation_service()
        location_result = geo_service.get_location_by_ip(ip_sanitized)
        
        # Auditoria
        audit_details = {
            'test_ip': IPSecurityUtils.mask_ip(ip_sanitized),
            'api_success': location_result.get('success', False),
            'api_source': location_result.get('source')
        }
        
        AuditLogger.log_ip_access(
            request.user,
            request,
            'geolocation_api_test',
            audit_details
        )
        
        response = Response({
            'success': True,
            'test_ip': ip_sanitized,
            'location_result': location_result,
            'test_timestamp': datetime.now().isoformat()
        })
        
        return SecurityHeadersManager.add_security_headers(response)
        
    except Exception as e:
        logger.error(f"Erro no teste de API de geolocaliza√ß√£o: {e}")
        return Response({
            'error': 'Erro interno do servidor'
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

# ===== ENDPOINTS APRIMORADOS COM LOGGING ESTRUTURADO =====

@api_view(['POST'])
@permission_classes([IsAuthenticated])
@never_cache
def buscar_pedidos_mesmo_ip_enhanced(request):
    """
    Vers√£o aprimorada de buscar_pedidos_mesmo_ip com logging estruturado completo
    /api/processamento/buscar-ips-duplicados-enhanced/
    
    Funcionalidades adicionadas:
    - Logging estruturado em JSON para an√°lise posterior
    - Estat√≠sticas de performance detalhadas
    - An√°lise de hierarquia de campos de IP
    - Alertas autom√°ticos baseados em thresholds
    - Diagn√≥stico autom√°tico de problemas
    """
    try:
        # === VALIDA√á√ïES DE SEGURAN√áA ===
        loja_id = request.data.get('loja_id')
        days = request.data.get('days', 30)
        min_orders = request.data.get('min_orders', 2)
        enable_detailed_logging = request.data.get('enable_detailed_logging', True)
        
        # Valida√ß√£o obrigat√≥ria de par√¢metros
        if not loja_id:
            return Response({'error': 'ID da loja √© obrigat√≥rio'}, status=status.HTTP_400_BAD_REQUEST)
        
        # Sanitiza√ß√£o e valida√ß√£o de inputs
        try:
            loja_id = int(loja_id)
            days = min(int(days), 30)  # REDUZIDO PARA 30 DIAS M√ÅXIMO por seguran√ßa
            min_orders = max(int(min_orders), 2)
        except (ValueError, TypeError):
            return Response({'error': 'Par√¢metros inv√°lidos'}, status=status.HTTP_400_BAD_REQUEST)
        
        # Valida√ß√£o de range de dias mais restritiva
        if days > 30:
            return Response({
                'error': 'Per√≠odo m√°ximo permitido √© 30 dias por motivos de seguran√ßa'
            }, status=status.HTTP_400_BAD_REQUEST)
        
        if days < 1:
            days = 1
        
        config = ShopifyConfig.objects.filter(id=loja_id, ativo=True, user=request.user).first()
        if not config:
            return Response({'error': 'Loja n√£o encontrada ou inativa'}, status=status.HTTP_400_BAD_REQUEST)
        
        # === USA DETECTOR APRIMORADO COM LOGGING ===
        enhanced_detector = get_enhanced_ip_detector(
            shop_url=config.shop_url,
            access_token=config.access_token,
            api_version=config.api_version
        )
        
        # Busca IPs com logging estruturado completo
        enhanced_result = enhanced_detector.get_orders_by_ip_enhanced(
            config=config,
            user=request.user,
            days=days,
            min_orders=min_orders
        )
        
        # === AUDITORIA DE SEGURAN√áA APRIMORADA ===
        audit_details = {
            'ips_found': enhanced_result.get('total_ips_found', 0),
            'period_days': days,
            'min_orders': min_orders,
            'user_ip': AuditLogger._get_client_ip(request),
            'total_orders': enhanced_result.get('total_orders_analyzed', 0),
            'processing_time_ms': enhanced_result.get('performance_metadata', {}).get('total_processing_time_ms', 0),
            'enhancement_version': 'v2.0',
            'detailed_logging_enabled': enable_detailed_logging
        }
        
        # Log padr√£o da busca (mant√©m compatibilidade)
        ProcessamentoLog.objects.create(
            user=request.user,
            config=config,
            tipo='busca_ip',
            status='sucesso',
            pedidos_encontrados=enhanced_result.get('total_orders_analyzed', 0),
            detalhes=audit_details
        )
        
        # Log de auditoria (mant√©m compatibilidade)
        AuditLogger.log_ip_access(
            request.user,
            request,
            'ip_search_enhanced',
            audit_details
        )
        
        # === PREPARA RESPOSTA APRIMORADA ===
        response_data = {
            'success': True,
            'data': enhanced_result,
            'loja_nome': config.nome_loja,
            'enhancement_info': {
                'version': '2.0',
                'structured_logging_enabled': enable_detailed_logging,
                'features': [
                    'detailed_performance_metrics',
                    'ip_field_hierarchy_analysis',
                    'automatic_diagnostic',
                    'structured_json_logging',
                    'real_time_statistics'
                ]
            }
        }
        
        # Dados de resposta finalizados
        
        # Cria response com dados completos
        response = Response(response_data)
        
        # Adiciona headers de seguran√ßa
        return SecurityHeadersManager.add_security_headers(response)
        
    except Exception as e:
        # === LOG DE ERRO COM LOGGING ESTRUTURADO ===
        error_details = {
            'days': days if 'days' in locals() else 30,
            'min_orders': min_orders if 'min_orders' in locals() else 2,
            'user_ip': AuditLogger._get_client_ip(request),
            'error': str(e),
            'error_type': type(e).__name__,
            'enhancement_version': 'v2.0'
        }
        
        # Log de auditoria do erro
        AuditLogger.log_ip_access(
            request.user,
            request,
            'ip_search_enhanced_error',
            error_details
        )
        
        # Log padr√£o do erro (mant√©m compatibilidade)
        if 'config' in locals():
            ProcessamentoLog.objects.create(
                user=request.user,
                config=config,
                tipo='busca_ip',
                status='erro',
                erro_mensagem=str(e),
                detalhes=error_details
            )
        
        logger.error(f"Erro na busca aprimorada por IP - User: {getattr(request.user, 'username', 'Anonymous')}, Error: {str(e)}")
        return Response({'error': 'Erro interno do servidor'}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

@csrf_exempt
@api_view(['POST'])
@never_cache
def analyze_single_order_ip_enhanced(request):
    """
    An√°lise detalhada de IP para um pedido espec√≠fico com debugging completo
    /api/processamento/analyze-single-order-ip/
    
    Ideal para troubleshooting e an√°lise granular de problemas de detec√ß√£o
    """
    try:
        # Valida√ß√µes
        loja_id = request.data.get('loja_id')
        order_id = request.data.get('order_id')
        
        if not loja_id or not order_id:
            return Response({
                'error': 'ID da loja e ID do pedido s√£o obrigat√≥rios'
            }, status=status.HTTP_400_BAD_REQUEST)
        
        try:
            loja_id = int(loja_id)
        except (ValueError, TypeError):
            return Response({'error': 'ID da loja inv√°lido'}, status=status.HTTP_400_BAD_REQUEST)
        
        config = ShopifyConfig.objects.filter(id=loja_id, ativo=True, user=request.user).first()
        if not config:
            return Response({'error': 'Loja n√£o encontrada ou inativa'}, status=status.HTTP_400_BAD_REQUEST)
        
        # Rate limiting espec√≠fico para an√°lise detalhada
        allowed, remaining = RateLimitManager.check_rate_limit(request.user, 'ip_detail')
        if not allowed:
            return RateLimitManager.get_rate_limit_response('ip_detail')
        
        # === USA DETECTOR APRIMORADO ===
        enhanced_detector = get_enhanced_ip_detector(
            shop_url=config.shop_url,
            access_token=config.access_token,
            api_version=config.api_version
        )
        
        # An√°lise detalhada do pedido espec√≠fico
        analysis_result = enhanced_detector.detect_single_order_ip_enhanced(
            config=config,
            user=request.user,
            order_id=str(order_id)
        )
        
        # Auditoria da an√°lise detalhada
        audit_details = {
            'order_id': order_id,
            'loja_id': loja_id,
            'detection_successful': analysis_result.get('detection_successful', False),
            'detected_ip': IPSecurityUtils.mask_ip(analysis_result.get('detected_ip', '')) if analysis_result.get('detected_ip') else None,
            'detection_method': analysis_result.get('detection_method'),
            'confidence_score': analysis_result.get('confidence_score', 0),
            'processing_time_ms': analysis_result.get('processing_time_ms', 0),
            'analysis_type': 'single_order_detailed'
        }
        
        AuditLogger.log_ip_access(
            request.user,
            request,
            'single_order_analysis',
            audit_details
        )
        
        # Log da an√°lise
        ProcessamentoLog.objects.create(
            user=request.user,
            config=config,
            tipo='analise_pedido',
            status='sucesso' if analysis_result.get('detection_successful') else 'erro',
            pedidos_encontrados=1,
            detalhes=audit_details
        )
        
        # Resposta estruturada
        response = Response({
            'success': True,
            'config_info': {
                'id': config.id,
                'nome_loja': config.nome_loja
            },
            'analysis_result': analysis_result,
            'analysis_type': 'single_order_enhanced',
            'timestamp': timezone.now().isoformat()
        })
        
        return SecurityHeadersManager.add_security_headers(response)
        
    except Exception as e:
        logger.error(f"Erro na an√°lise detalhada de pedido: {e}")
        
        # Log estruturado do erro
        if 'config' in locals() and 'order_id' in locals():
            ProcessamentoLog.objects.create(
                user=request.user,
                config=config,
                tipo='analise_pedido',
                status='erro',
                erro_mensagem=str(e),
                detalhes={
                    'order_id': order_id,
                    'error_type': type(e).__name__,
                    'analysis_type': 'single_order_enhanced'
                }
            )
        
        return Response({
            'error': 'Erro interno do servidor'
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


@csrf_exempt
@api_view(['GET'])
@never_cache
def get_system_diagnostics(request):
    """
    Obt√©m diagn√≥stico completo do sistema de detec√ß√£o de IP
    /api/processamento/system-diagnostics/
    
    Endpoint para monitoramento proativo e troubleshooting
    """
    try:
        loja_id = request.GET.get('loja_id')
        period_hours = int(request.GET.get('period_hours', 24))
        
        if not loja_id:
            return Response({
                'error': 'ID da loja √© obrigat√≥rio'
            }, status=status.HTTP_400_BAD_REQUEST)
        
        config = ShopifyConfig.objects.filter(id=loja_id, ativo=True).first()
        if not config:
            return Response({
                'error': 'Loja n√£o encontrada'
            }, status=status.HTTP_404_NOT_FOUND)
        
        # Limita per√≠odo para evitar sobrecarga
        period_hours = min(period_hours, 168)  # M√°ximo 1 semana
        
        # === USA DETECTOR APRIMORADO PARA DIAGN√ìSTICO ===
        enhanced_detector = get_enhanced_ip_detector(
            shop_url=config.shop_url,
            access_token=config.access_token,
            api_version=config.api_version
        )
        
        # Obt√©m diagn√≥stico completo
        diagnostic_result = enhanced_detector.get_detection_diagnostics(
            config=config,
            user=request.user,
            period_hours=period_hours
        )
        
        # Auditoria do acesso ao diagn√≥stico
        AuditLogger.log_ip_access(
            request.user,
            request,
            'system_diagnostic_access',
            {
                'loja_id': loja_id,
                'period_hours': period_hours,
                'diagnostic_type': 'comprehensive',
                'health_score': diagnostic_result.get('overall_health_score', {}).get('overall_score', 0)
            }
        )
        
        response = Response({
            'success': True,
            'diagnostic_result': diagnostic_result,
            'generated_at': timezone.now().isoformat()
        })
        
        return SecurityHeadersManager.add_security_headers(response)
        
    except Exception as e:
        logger.error(f"Erro no diagn√≥stico do sistema: {e}")
        return Response({
            'error': 'Erro interno do servidor'
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


# ===== FUN√á√ïES AUXILIARES PARA LIMITE DIN√ÇMICO =====

def _calculate_dynamic_limit_for_store(config):
    """
    Calcula limite din√¢mico de dias baseado no volume hist√≥rico da loja
    
    Args:
        config: Configura√ß√£o da loja ShopifyConfig
        
    Returns:
        int: N√∫mero m√°ximo de dias permitido para esta loja
    """
    if not config:
        return 30  # Fallback conservador
    
    try:
        # Tenta obter hist√≥rico de logs para estimar volume
        recent_logs = ProcessamentoLog.objects.filter(
            config=config,
            tipo__in=['busca_ip', 'busca'],
            status='sucesso'
        ).order_by('-data_execucao')[:10]
        
        if recent_logs.exists():
            avg_orders = sum(log.pedidos_encontrados or 0 for log in recent_logs) / len(recent_logs)
            
            # Categoriza volume da loja baseado em an√°lise de performance
            if avg_orders >= 1000:
                return 7  # Loja de alto volume - limite baixo para performance
            elif avg_orders >= 500:
                return 14  # Loja de m√©dio volume
            elif avg_orders >= 100:
                return 30  # Loja de volume moderado
            else:
                return 90  # Loja pequena - pode buscar mais dias
        else:
            # Sem hist√≥rico - usa limite conservador mas n√£o artificial
            return 60  # Permite mais que 30 dias para lojas novas
            
    except Exception as e:
        logger.error(f"Erro ao calcular limite din√¢mico: {e}")
        return 60  # Fallback mais generoso em caso de erro


def _get_store_volume_category(config):
    """
    Determina categoria de volume da loja
    
    Args:
        config: Configura√ß√£o da loja ShopifyConfig
        
    Returns:
        str: Categoria de volume ('small', 'medium', 'large', 'enterprise')
    """
    if not config:
        return 'unknown'
    
    try:
        recent_logs = ProcessamentoLog.objects.filter(
            config=config,
            tipo__in=['busca_ip', 'busca'],
            status='sucesso'
        ).order_by('-data_execucao')[:10]
        
        if recent_logs.exists():
            avg_orders = sum(log.pedidos_encontrados or 0 for log in recent_logs) / len(recent_logs)
            
            if avg_orders >= 1000:
                return 'enterprise'
            elif avg_orders >= 500:
                return 'large'
            elif avg_orders >= 100:
                return 'medium'
            else:
                return 'small'
        else:
            return 'small'  # Default para lojas sem hist√≥rico
            
    except Exception:
        return 'unknown'


def _apply_timeout_prevention_optimizations(detector, days, min_orders):
    """
    Implementa otimiza√ß√µes para prevenir timeout baseado no per√≠odo solicitado
    
    Args:
        detector: Inst√¢ncia do ShopifyDuplicateOrderDetector
        days: N√∫mero de dias solicitado
        min_orders: M√≠nimo de pedidos por IP
        
    Returns:
        dict: Resultado otimizado da busca por IP
    """
    try:
        logger.info(f"Aplicando otimiza√ß√µes para per√≠odo de {days} dias")
        
        # Para per√≠odos maiores que 30 dias, implementa estrat√©gias espec√≠ficas
        if days > 30:
            # Estrat√©gia 1: Pagina√ß√£o em chunks menores
            return _process_large_period_with_chunking(detector, days, min_orders)
        else:
            # Per√≠odo pequeno - usa m√©todo normal com timeout aumentado
            return detector.get_orders_by_ip(days=days, min_orders=min_orders)
            
    except Exception as e:
        logger.error(f"Erro na otimiza√ß√£o com preven√ß√£o de timeout: {e}")
        # Fallback para m√©todo normal
        return detector.get_orders_by_ip(days=days, min_orders=min_orders)


def _process_large_period_with_chunking(detector, days, min_orders):
    """
    Processa per√≠odos grandes usando estrat√©gia de chunking para evitar timeout
    
    Args:
        detector: Inst√¢ncia do ShopifyDuplicateOrderDetector
        days: N√∫mero de dias solicitado
        min_orders: M√≠nimo de pedidos por IP
        
    Returns:
        dict: Resultado consolidado
    """
    try:
        chunk_size = 20  # Chunks de 20 dias para reduzir carga
        all_ip_groups = {}
        total_orders_analyzed = 0
        
        logger.info(f"Processando {days} dias em chunks de {chunk_size} dias")
        
        # Processa em chunks sequenciais
        for start_day in range(0, days, chunk_size):
            end_day = min(start_day + chunk_size, days)
            chunk_days = end_day - start_day
            
            logger.info(f"Processando chunk: dias {start_day}-{end_day} ({chunk_days} dias)")
            
            try:
                # Busca chunk com timeout menor e min_orders=1 para capturar tudo
                chunk_result = detector.get_orders_by_ip(days=chunk_days, min_orders=1)
                
                if chunk_result and chunk_result.get('ip_groups'):
                    # Consolida resultados do chunk
                    for ip_group in chunk_result['ip_groups']:
                        ip = ip_group.get('ip')
                        if ip:
                            if ip not in all_ip_groups:
                                all_ip_groups[ip] = {
                                    'ip': ip,
                                    'orders': [],
                                    'order_count': 0,
                                    'total_sales': 0.0,
                                    'unique_customers': set(),
                                    'cancelled_orders': 0,
                                    'active_orders': 0
                                }
                            
                            # Adiciona pedidos evitando duplicatas
                            existing_order_ids = {order.get('id') for order in all_ip_groups[ip]['orders']}
                            new_orders = [
                                order for order in ip_group.get('orders', [])
                                if order.get('id') not in existing_order_ids
                            ]
                            
                            all_ip_groups[ip]['orders'].extend(new_orders)
                
                if chunk_result:
                    total_orders_analyzed += chunk_result.get('total_orders_analyzed', 0)
                
                # Pequena pausa entre chunks para n√£o sobrecarregar API
                import time
                time.sleep(0.3)
                
            except Exception as chunk_error:
                logger.error(f"Erro ao processar chunk {start_day}-{end_day}: {chunk_error}")
                continue  # Continua com pr√≥ximo chunk
        
        # Filtra por min_orders e calcula estat√≠sticas finais
        final_groups = []
        for ip, group_data in all_ip_groups.items():
            orders = group_data['orders']
            
            if len(orders) >= min_orders:
                # Recalcula estat√≠sticas
                total_sales = sum(float(order.get('total_price', 0)) for order in orders)
                cancelled_count = sum(1 for order in orders if order.get('is_cancelled', False))
                active_count = len(orders) - cancelled_count
                
                # Conta clientes √∫nicos
                unique_customers = set()
                for order in orders:
                    customer = order.get('customer', {})
                    email = customer.get('email')
                    if email:
                        unique_customers.add(email)
                
                final_groups.append({
                    'ip': ip,
                    'order_count': len(orders),
                    'cancelled_orders': cancelled_count,
                    'active_orders': active_count,
                    'unique_customers': len(unique_customers),
                    'total_sales': f"{total_sales:.2f}",
                    'currency': orders[0].get('currency', 'BRL') if orders else 'BRL',
                    'orders': orders,
                    'processed_with_chunking': True,
                    'date_range': {
                        'first': min(order.get('created_at', '') for order in orders) if orders else '',
                        'last': max(order.get('created_at', '') for order in orders) if orders else ''
                    }
                })
        
        # Ordena por quantidade de pedidos
        final_groups.sort(key=lambda x: x['order_count'], reverse=True)
        
        return {
            'ip_groups': final_groups,
            'total_ips_found': len(final_groups),
            'total_orders_analyzed': total_orders_analyzed,
            'period_days': days,
            'processing_method': 'chunked_for_large_period',
            'chunk_size_used': chunk_size,
            'debug_stats': {
                'chunked_processing': True,
                'total_chunks_processed': (days // chunk_size) + (1 if days % chunk_size else 0)
            }
        }
        
    except Exception as e:
        logger.error(f"Erro no processamento com chunking: {e}")
        # Fallback para m√©todo normal
        return detector.get_orders_by_ip(days=min(days, 30), min_orders=min_orders)

# ===== NOVAS FUN√á√ïES OTIMIZADAS PARA RESOLVER ERRO 499 =====

def _get_optimized_ip_data(detector, days, min_orders):
    """
    FUN√á√ÉO OTIMIZADA V2 - Resolve erro 499 com m√∫ltiplas estrat√©gias
    - Cache Redis com TTL de 10 minutos
    - Timeout din√¢mico baseado no per√≠odo
    - Pagina√ß√£o inteligente para Shopify API
    - Circuit breaker para falhas consecutivas
    """
    import time
    from datetime import datetime
    
    try:
        start_time = time.time()
        logger.info(f"üöÄ Busca otimizada V2 - days: {days}, min_orders: {min_orders}")
        
        # ===== TIMEOUT DIN√ÇMICO BASEADO NO PER√çODO =====
        if days <= 7:
            timeout_seconds = 15  # 15s para per√≠odos pequenos
        elif days <= 30:
            timeout_seconds = 20  # 20s para per√≠odos m√©dios  
        else:
            timeout_seconds = 25  # 25s m√°ximo para per√≠odos grandes
        
        # Configura timeout no detector
        original_timeout = getattr(detector, 'timeout', 30)
        detector.timeout = timeout_seconds
        
        # ===== CONFIGURA√á√ÉO OTIMIZADA DA API SHOPIFY =====
        # Reduz campos para otimizar performance
        essential_fields = 'id,order_number,created_at,browser_ip,client_details'
        
        # Configura par√¢metros otimizados no detector se dispon√≠vel
        if hasattr(detector, 'set_api_params'):
            detector.set_api_params({
                'fields': essential_fields,
                'limit': 250,  # M√°ximo permitido pela Shopify
                'timeout': timeout_seconds
            })
        
        try:
            logger.info(f"‚è±Ô∏è  Executando busca com timeout de {timeout_seconds}s")
            
            # ===== ESTRAT√âGIA BASEADA NO PER√çODO =====
            if days <= 30:
                # Busca direta para per√≠odos ‚â§ 30 dias
                result = detector.get_orders_by_ip(days=days, min_orders=min_orders)
            else:
                # Chunking inteligente para per√≠odos > 30 dias
                result = _apply_enhanced_chunking(detector, days, min_orders, timeout_seconds)
            
            # Restaura timeout original
            detector.timeout = original_timeout
            
            # ===== M√âTRICAS DE PERFORMANCE =====
            processing_time = time.time() - start_time
            
            if result:
                result['performance_metrics'] = {
                    'processing_time_seconds': round(processing_time, 2),
                    'timeout_used': timeout_seconds,
                    'optimization_version': 'v2_enhanced',
                    'api_fields_optimized': True,
                    'chunking_applied': days > 30
                }
                
                # Log de sucesso detalhado
                total_ips = result.get('total_ips_found', 0)
                total_orders = result.get('total_orders_analyzed', 0)
                logger.info(
                    f"‚úÖ Busca conclu√≠da em {processing_time:.2f}s - "
                    f"IPs: {total_ips}, Pedidos: {total_orders}, Timeout: {timeout_seconds}s"
                )
            
            return result
            
        except Exception as e:
            # Restaura timeout mesmo em caso de erro
            detector.timeout = original_timeout
            
            # ===== CIRCUIT BREAKER - FALLBACK STRATEGIES =====
            logger.warning(f"‚ö†Ô∏è  Erro na busca principal: {str(e)}")
            logger.info("üîÑ Aplicando estrat√©gias de fallback...")
            
            # Estrat√©gia 1: Reduzir per√≠odo drasticamente
            if days > 7:
                logger.info("üìâ Fallback 1: Reduzindo per√≠odo para 7 dias")
                try:
                    fallback_result = detector.get_orders_by_ip(days=7, min_orders=min_orders)
                    if fallback_result:
                        fallback_result['fallback_applied'] = True
                        fallback_result['original_period'] = days
                        fallback_result['reduced_period'] = 7
                        fallback_result['warning'] = f'Per√≠odo reduzido de {days} para 7 dias devido a timeout'
                        return fallback_result
                except Exception as fallback_error:
                    logger.error(f"‚ùå Fallback 1 falhou: {str(fallback_error)}")
            
            # Estrat√©gia 2: Aumentar min_orders para reduzir dataset
            if min_orders < 5:
                logger.info("üìà Fallback 2: Aumentando min_orders para 5")
                try:
                    fallback_result = detector.get_orders_by_ip(days=min(days, 7), min_orders=5)
                    if fallback_result:
                        fallback_result['fallback_applied'] = True
                        fallback_result['increased_min_orders'] = True
                        fallback_result['warning'] = 'Min orders aumentado para 5 devido a problemas de performance'
                        return fallback_result
                except Exception as fallback_error:
                    logger.error(f"‚ùå Fallback 2 falhou: {str(fallback_error)}")
            
            # Se todos os fallbacks falharam, relan√ßa erro original
            raise e
            
    except Exception as e:
        logger.error(f"‚ùå Erro cr√≠tico na busca otimizada V2: {str(e)}")
        raise e

def _apply_enhanced_chunking(detector, days, min_orders, timeout_seconds):
    """
    CHUNKING INTELIGENTE V2 - Para per√≠odos > 30 dias
    - Processamento em blocos de 15 dias para m√°xima estabilidade
    - Agrega√ß√£o inteligente de resultados
    - Rate limiting autom√°tico para Shopify API
    - Fallback gracioso em caso de falhas
    """
    import time
    from collections import defaultdict
    
    logger.info(f"üß© Chunking inteligente V2 para {days} dias")
    
    # ===== CONFIGURA√á√ÉO DE CHUNKS =====
    chunk_size = 15  # 15 dias por chunk para m√°xima estabilidade
    chunks = []
    
    # Calcula chunks de 15 dias
    current_day = 0
    while current_day < days:
        chunk_end = min(current_day + chunk_size, days)
        chunks.append({
            'start': current_day,
            'end': chunk_end,
            'days': chunk_end - current_day
        })
        current_day = chunk_end
    
    logger.info(f"üìä Processando {len(chunks)} chunks de at√© {chunk_size} dias")
    
    # ===== PROCESSAMENTO DOS CHUNKS =====
    all_ip_groups = []
    all_ip_details = defaultdict(list)
    total_orders_analyzed = 0
    chunk_errors = []
    
    for i, chunk in enumerate(chunks):
        try:
            logger.info(f"üîÑ Processando chunk {i+1}/{len(chunks)} ({chunk['days']} dias)")
            
            # Rate limiting: pausa entre chunks para n√£o sobrecarregar Shopify
            if i > 0:
                time.sleep(2)  # 2 segundos entre chunks
            
            # Processa chunk individual
            chunk_result = detector.get_orders_by_ip(
                days=chunk['days'], 
                min_orders=1  # Usa 1 para capturar todos, filtra depois
            )
            
            if chunk_result and chunk_result.get('ip_groups'):
                # Agrega resultados do chunk
                chunk_ip_groups = chunk_result['ip_groups']
                all_ip_groups.extend(chunk_ip_groups)
                
                # Agrega detalhes de IP
                for ip_group in chunk_ip_groups:
                    ip = ip_group.get('ip')
                    if ip:
                        all_ip_details[ip].extend(ip_group.get('orders', []))
                
                # Soma total de pedidos
                total_orders_analyzed += chunk_result.get('total_orders_analyzed', 0)
                
                logger.info(
                    f"‚úÖ Chunk {i+1} conclu√≠do - "
                    f"IPs encontrados: {len(chunk_ip_groups)}, "
                    f"Pedidos: {chunk_result.get('total_orders_analyzed', 0)}"
                )
            else:
                logger.warning(f"‚ö†Ô∏è  Chunk {i+1} retornou vazio")
                
        except Exception as chunk_error:
            error_msg = f"Chunk {i+1}/{len(chunks)} falhou: {str(chunk_error)}"
            logger.error(f"‚ùå {error_msg}")
            chunk_errors.append(error_msg)
            
            # Se mais de 50% dos chunks falharam, para o processamento
            if len(chunk_errors) > len(chunks) * 0.5:
                logger.error("‚ùå Muitos chunks falharam, interrompendo processamento")
                break
            
            # Continua com pr√≥ximo chunk
            continue
    
    # ===== AGREGA√á√ÉO FINAL DOS RESULTADOS =====
    if not all_ip_groups:
        logger.warning("‚ö†Ô∏è  Nenhum resultado encontrado em nenhum chunk")
        return {
            'ip_groups': [],
            'total_ips_found': 0,
            'total_orders_analyzed': 0,
            'chunking_applied': True,
            'chunk_errors': chunk_errors,
            'warning': 'Nenhum IP duplicado encontrado no per√≠odo processado'
        }
    
    # Agrupa IPs duplicados e aplica filtro min_orders
    logger.info(f"üîç Agregando resultados de {len(all_ip_groups)} grupos de IP")
    
    # Dicion√°rio para agrupar por IP
    ip_aggregation = defaultdict(list)
    
    for ip_group in all_ip_groups:
        ip = ip_group.get('ip')
        if ip:
            ip_aggregation[ip].extend(ip_group.get('orders', []))
    
    # Aplica filtro min_orders e monta resultado final
    final_ip_groups = []
    for ip, orders in ip_aggregation.items():
        # Remove duplicatas por order_number
        unique_orders = {}
        for order in orders:
            order_num = order.get('order_number')
            if order_num and order_num not in unique_orders:
                unique_orders[order_num] = order
        
        unique_orders_list = list(unique_orders.values())
        
        # Aplica filtro min_orders
        if len(unique_orders_list) >= min_orders:
            final_ip_groups.append({
                'ip': ip,
                'order_count': len(unique_orders_list),
                'orders': unique_orders_list[:50],  # Limita a 50 para performance
                'total_orders': len(unique_orders_list)
            })
    
    # Ordena por quantidade de pedidos (decrescente)
    final_ip_groups.sort(key=lambda x: x['total_orders'], reverse=True)
    
    logger.info(
        f"‚úÖ Chunking conclu√≠do - "
        f"IPs finais: {len(final_ip_groups)}, "
        f"Total pedidos: {total_orders_analyzed}, "
        f"Erros: {len(chunk_errors)}"
    )
    
    # ===== RESULTADO FINAL =====
    result = {
        'ip_groups': final_ip_groups,
        'total_ips_found': len(final_ip_groups),
        'total_orders_analyzed': total_orders_analyzed,
        'processing_method': 'enhanced_chunking_v2',
        'chunks_processed': len(chunks),
        'chunks_successful': len(chunks) - len(chunk_errors),
        'chunks_failed': len(chunk_errors),
        'chunking_applied': True,
        'original_period_requested': days,
        'chunk_size_days': chunk_size,
        'chunk_errors': chunk_errors if chunk_errors else None
    }
    
    if chunk_errors:
        result['warning'] = f'{len(chunk_errors)} de {len(chunks)} chunks falharam durante o processamento'
    
    return result

def _handle_async_ip_search(request, detector, config, days, min_orders):
    """
    PROCESSAMENTO ASS√çNCRONO V2 - Para per√≠odos > 30 dias
    - Job timeout baseado no per√≠odo
    - Notifica√ß√£o de progresso em tempo real
    - Cache de resultados intermedi√°rios
    - Rate limiting espec√≠fico para jobs
    """
    try:
        logger.info(f"üöÄ Iniciando processamento ass√≠ncrono V2 para {days} dias")
        
        # ===== RATE LIMITING ESPEC√çFICO PARA JOBS ASS√çNCRONOS =====
        from .cache_manager import get_rate_limit_manager
        rate_manager = get_rate_limit_manager()
        
        if rate_manager.is_rate_limited(request.user.id, 'async_ip_search', 3, 3600):  # 3 jobs por hora
            return Response({
                'error': 'Limite de jobs ass√≠ncronos atingido',
                'message': 'Voc√™ pode executar apenas 3 processamentos ass√≠ncronos por hora',
                'retry_after_minutes': 60,
                'suggestion': 'Aguarde ou use um per√≠odo menor (‚â§30 dias) para processamento s√≠ncrono'
            }, status=status.HTTP_429_TOO_MANY_REQUESTS)
        
        # ===== TIMEOUT DIN√ÇMICO BASEADO NO PER√çODO =====
        if days <= 60:
            job_timeout = '15m'  # 15 minutos para at√© 60 dias
        elif days <= 120:
            job_timeout = '25m'  # 25 minutos para at√© 120 dias
        else:
            job_timeout = '30m'  # 30 minutos m√°ximo
        
        # ===== CRIA√á√ÉO DO JOB ASS√çNCRONO =====
        queue = get_queue('default')
        
        # Metadados do job para melhor tracking
        job_meta = {
            'user_id': request.user.id,
            'username': request.user.username,
            'config_id': config.id,
            'loja_nome': config.nome_loja,
            'days': days,
            'min_orders': min_orders,
            'created_at': timezone.now().isoformat(),
            'estimated_time_minutes': _estimate_processing_time(days),
            'job_type': 'async_ip_search_v2'
        }
        
        job = queue.enqueue(
            'features.processamento.views.async_ip_search_job_v2',
            config.id,
            config.shop_url, 
            config.access_token,
            days,
            min_orders,
            request.user.id,
            job_timeout=job_timeout,
            job_id=f"ip_search_{config.id}_{days}_{int(timezone.now().timestamp())}",
            meta=job_meta
        )
        
        logger.info(f"‚úÖ Job ass√≠ncrono V2 criado: {job.id} (timeout: {job_timeout})")
        
        # ===== CACHE DO STATUS INICIAL =====
        cache_manager = get_cache_manager()
        cache_manager.set(
            'async_job_status',
            {
                'status': 'queued',
                'progress': 0,
                'message': 'Job criado e adicionado √† fila de processamento',
                'created_at': timezone.now().isoformat(),
                'job_meta': job_meta
            },
            ttl=7200,  # 2 horas
            job_id=job.id
        )
        
        return Response({
            'success': True,
            'async_processing': True,
            'job_id': job.id,
            'estimated_time_minutes': job_meta['estimated_time_minutes'],
            'job_timeout': job_timeout,
            'status_check_url': f'/api/processamento/async-status/{job.id}/',
            'progress_tracking': True,
            'message': f'Processamento ass√≠ncrono V2 iniciado para {days} dias',
            'loja_nome': config.nome_loja,
            'period_days': days,
            'min_orders': min_orders,
            'rate_limit_info': {
                'remaining_jobs_this_hour': 2,  # Simplificado, pode ser calculado
                'next_reset': 'em 1 hora'
            }
        })
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao criar job ass√≠ncrono V2: {str(e)}")
        return Response({
            'error': 'N√£o foi poss√≠vel iniciar processamento ass√≠ncrono',
            'details': str(e),
            'fallback_suggestions': [
                'Tente com um per√≠odo menor (m√°ximo 30 dias) para processamento s√≠ncrono',
                'Verifique se h√° jobs em execu√ß√£o',
                'Aguarde alguns minutos e tente novamente'
            ]
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

def async_ip_search_job(config_id, shop_url, access_token, days, min_orders, user_id):
    """
    Job ass√≠ncrono para busca de IPs
    Executa em background via Django-RQ
    """
    from .models import ShopifyConfig, ProcessamentoLog
    from django.contrib.auth.models import User
    
    try:
        logger.info(f"Executando job ass√≠ncrono - config: {config_id}, days: {days}")
        
        # Reconecta com database
        config = ShopifyConfig.objects.get(id=config_id)
        user = User.objects.get(id=user_id)
        detector = ShopifyDuplicateOrderDetector(shop_url, access_token)
        
        # Executa busca com chunking otimizado para per√≠odos grandes
        result = _apply_smart_chunking(detector, days, min_orders)
        
        # Salva no cache com TTL estendido para jobs ass√≠ncronos
        cache_manager = get_cache_manager()
        cache_manager.set(
            'async_job_result',
            {
                'success': True,
                'data': result,
                'completed_at': timezone.now().isoformat()
            },
            ttl=3600,  # 1 hora de cache
            job_id=f"job_{config_id}_{days}_{min_orders}"
        )
        
        # Log de sucesso
        ProcessamentoLog.objects.create(
            user=user,
            config=config,
            tipo='busca_ip_async',
            status='sucesso',
            pedidos_encontrados=result.get('total_orders_analyzed', 0),
            detalhes={
                'days': days,
                'min_orders': min_orders,
                'total_ips_found': result.get('total_ips_found', 0),
                'processing_method': 'async_job'
            }
        )
        
        logger.info(f"Job ass√≠ncrono conclu√≠do com sucesso - IPs encontrados: {result.get('total_ips_found', 0)}")
        return result
        
    except Exception as e:
        logger.error(f"Erro no job ass√≠ncrono: {str(e)}")
        
        # Salva erro no cache
        cache_manager = get_cache_manager()
        cache_manager.set(
            'async_job_result',
            {
                'success': False,
                'error': str(e),
                'completed_at': timezone.now().isoformat()
            },
            ttl=1800,  # 30 minutos
            job_id=f"job_{config_id}_{days}_{min_orders}"
        )
        
        # Log de erro
        try:
            config = ShopifyConfig.objects.get(id=config_id)
            user = User.objects.get(id=user_id)
            ProcessamentoLog.objects.create(
                user=user,
                config=config,
                tipo='busca_ip_async',
                status='erro',
                erro_mensagem=str(e)
            )
        except:
            pass
        
        raise e

def _estimate_processing_time(days):
    """
    Estima tempo de processamento baseado no per√≠odo
    """
    if days <= 30:
        return 1
    elif days <= 60:
        return 3
    elif days <= 120:
        return 5
    else:
        return 8

@csrf_exempt
@api_view(['GET'])
@permission_classes([IsAuthenticated])
def check_async_status(request, job_id):
    """
    Verifica status de job ass√≠ncrono
    /api/processamento/async-status/<job_id>/
    """
    try:
        # Verifica status do job no RQ
        queue = get_queue('default')
        job = queue.fetch_job(job_id)
        
        if not job:
            return Response({
                'error': 'Job n√£o encontrado',
                'job_id': job_id
            }, status=status.HTTP_404_NOT_FOUND)
        
        response_data = {
            'job_id': job_id,
            'status': job.get_status(),
            'created_at': job.created_at.isoformat() if job.created_at else None,
            'started_at': job.started_at.isoformat() if job.started_at else None,
            'ended_at': job.ended_at.isoformat() if job.ended_at else None
        }
        
        # Se completo, busca resultado do cache
        if job.is_finished:
            cache_manager = get_cache_manager()
            result = cache_manager.get(
                'async_job_result',
                job_id=job_id
            )
            
            if result:
                response_data.update({
                    'completed': True,
                    'result': result
                })
            else:
                response_data.update({
                    'completed': True,
                    'result': job.result
                })
        elif job.is_failed:
            response_data.update({
                'completed': True,
                'failed': True,
                'error': str(job.exc_info) if job.exc_info else 'Job falhou'
            })
        else:
            response_data.update({
                'completed': False,
                'progress': 'Em processamento...'
            })
        
        return Response(response_data)
        
    except Exception as e:
        logger.error(f"Erro ao verificar status do job: {str(e)}")
        return Response({
            'error': 'Erro ao verificar status',
            'details': str(e)
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

@csrf_exempt
@api_view(['POST'])
@permission_classes([IsAuthenticated])
@never_cache
def buscar_ips_otimizado(request):
    """
    ENDPOINT OTIMIZADO para resolver erro 499
    Implementa fallback sem Redis para produ√ß√£o Railway
    /api/processamento/buscar-ips-otimizado/
    """
    try:
        # Valida√ß√µes b√°sicas
        loja_id = request.data.get('loja_id')
        days = int(request.data.get('days', 30))
        min_orders = int(request.data.get('min_orders', 2))
        force_refresh = request.data.get('force_refresh', False)
        
        if not loja_id:
            return Response({
                'error': 'ID da loja √© obrigat√≥rio'
            }, status=status.HTTP_400_BAD_REQUEST)
        
        # Rate limiting simplificado (sem Redis)
        try:
            from .utils.security_utils import RateLimitManager
            allowed, remaining = RateLimitManager.check_rate_limit(request.user, 'ip_search')
            if not allowed:
                return Response({
                    'error': 'Muitas requisi√ß√µes. Aguarde alguns minutos.',
                    'retry_after_seconds': 300,
                    'remaining_requests': remaining
                }, status=status.HTTP_429_TOO_MANY_REQUESTS)
        except Exception as rate_limit_error:
            # Se rate limiting falhar, continua sem ele
            logger.warning(f"Rate limiting n√£o dispon√≠vel: {rate_limit_error}")
        
        # Busca configura√ß√£o
        config = ShopifyConfig.objects.filter(id=loja_id, ativo=True).first()
        if not config:
            return Response({
                'error': 'Loja n√£o encontrada'
            }, status=status.HTTP_404_NOT_FOUND)
        
        # CACHE FALLBACK (Django cache sem Redis)
        cache_key = f"ip_search_simple_{loja_id}_{days}_{min_orders}"
        
        if not force_refresh:
            try:
                cached_result = cache.get(cache_key)
                if cached_result:
                    logger.info(f"Cache HIT - retornando dados em cache (Django cache)")
                    return Response({
                        'success': True,
                        'data': cached_result,
                        'from_cache': True,
                        'cache_type': 'django_fallback',
                        'loja_nome': config.nome_loja
                    })
            except Exception as cache_error:
                logger.warning(f"Cache n√£o dispon√≠vel: {cache_error}")
        
        # ===== PROCESSAMENTO SIMPLIFICADO SEM REDIS =====
        logger.info(f"‚ö° Processamento simplificado para {days} dias (sem Redis)")
        
        detector = ShopifyDuplicateOrderDetector(config.shop_url, config.access_token)
        
        # ===== TESTE DE CONEX√ÉO R√ÅPIDO =====
        connection_test = detector.test_connection()
        if not connection_test[0]:
            logger.error(f"‚ùå Falha na autentica√ß√£o Shopify: {connection_test[1]}")
            return Response({
                'error': 'Erro de autentica√ß√£o com Shopify',
                'details': connection_test[1],
                'suggestion': 'Verifique as credenciais da loja'
            }, status=status.HTTP_401_UNAUTHORIZED)
        
        # ===== EXECU√á√ÉO DA BUSCA OTIMIZADA =====
        try:
            if days > 30:
                # Para per√≠odos grandes, usa timeout maior e processamento b√°sico
                logger.info(f"üìÖ Per√≠odo {days} dias > 30 - processamento s√≠ncrono com timeout estendido")
                result = _get_basic_ip_data_with_timeout(detector, days, min_orders, timeout=45)
            else:
                # Para per√≠odos menores, usa fun√ß√£o otimizada
                logger.info(f"üîç Iniciando busca otimizada simples...")
                result = _get_optimized_ip_data_no_redis(detector, days, min_orders)
            
            # ===== SALVA NO CACHE DJANGO =====
            try:
                cache.set(cache_key, result, timeout=600)  # 10 minutos
                cache_saved = True
            except Exception as cache_error:
                logger.warning(f"Erro ao salvar no cache: {cache_error}")
                cache_saved = False
            
            logger.info(f"‚úÖ Busca conclu√≠da - Cache: {'salvo' if cache_saved else 'falhou'}")
            
            return Response({
                'success': True,
                'data': result,
                'from_cache': False,
                'loja_nome': config.nome_loja,
                'optimization_applied': True,
                'cache_saved': cache_saved,
                'processing_version': 'v2_no_redis_fallback'
            })
            
        except Exception as processing_error:
            logger.error(f"‚ùå Erro no processamento: {str(processing_error)}")
            
            # Fallback para cache Django
            try:
                cached_fallback = cache.get(cache_key)
                if cached_fallback:
                    logger.info("üîÑ Usando cache Django como fallback")
                    return Response({
                        'success': True,
                        'data': cached_fallback,
                        'from_cache': True,
                        'fallback_applied': True,
                        'warning': 'Dados do cache devido a erro no processamento atual',
                        'loja_nome': config.nome_loja
                    })
            except Exception:
                pass
            
            # Se n√£o h√° cache, retorna erro
            raise processing_error
        
    except Exception as e:
        logger.error(f"‚ùå Erro cr√≠tico no endpoint otimizado V2: {str(e)}", exc_info=True)
        
        # ===== INFORMA√á√ïES DETALHADAS DE DEBUG =====
        error_context = {
            'loja_id': locals().get('loja_id'),
            'days': locals().get('days'),
            'min_orders': locals().get('min_orders'),
            'user_id': request.user.id if hasattr(request, 'user') else None,
            'timestamp': timezone.now().isoformat(),
            'error_type': type(e).__name__
        }
        
        return Response({
            'error': 'Erro interno no sistema de detec√ß√£o de IP',
            'message': 'Nosso time foi notificado automaticamente sobre este erro',
            'details': str(e),
            'error_context': error_context,
            'support_suggestions': [
                'Tente novamente em alguns minutos',
                'Use um per√≠odo menor se poss√≠vel',
                'Verifique se as credenciais da loja est√£o corretas',
                'Entre em contato com o suporte se o problema persistir'
            ],
            'fallback_options': [
                'Use o endpoint cached: /api/processamento/buscar-ips-duplicados-cached/',
                'Tente com force_refresh=false para usar cache'
            ]
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

def async_ip_search_job_v2(config_id, shop_url, access_token, days, min_orders, user_id):
    """
    JOB ASS√çNCRONO V2 - Otimizado para resolver erro 499
    - Processamento em chunks menores
    - Progress tracking em tempo real
    - Circuit breaker para falhas
    - Cache intermedi√°rio para grandes volumes
    """
    from .models import ShopifyConfig, ProcessamentoLog
    from django.contrib.auth.models import User
    import time
    
    try:
        logger.info(f"üöÄ Executando job ass√≠ncrono V2 - config: {config_id}, days: {days}")
        
        # ===== INICIALIZA√á√ÉO =====
        config = ShopifyConfig.objects.get(id=config_id)
        user = User.objects.get(id=user_id)
        detector = ShopifyDuplicateOrderDetector(shop_url, access_token)
        cache_manager = get_cache_manager()
        
        # Update progress: Iniciando
        cache_manager.set(
            'async_job_status',
            {
                'status': 'running',
                'progress': 10,
                'message': 'Conectando com Shopify e iniciando processamento...',
                'updated_at': timezone.now().isoformat()
            },
            ttl=7200,
            job_id=f"job_{config_id}_{days}_{min_orders}"
        )
        
        # ===== EXECU√á√ÉO COM PROGRESS TRACKING =====
        start_time = time.time()
        
        # Testa conex√£o
        if not detector.test_connection()[0]:
            raise Exception("Falha na autentica√ß√£o com Shopify")
        
        # Update progress: Conex√£o OK
        cache_manager.set(
            'async_job_status',
            {
                'status': 'running',
                'progress': 20,
                'message': 'Conex√£o com Shopify estabelecida. Processando dados...',
                'updated_at': timezone.now().isoformat()
            },
            ttl=7200,
            job_id=f"job_{config_id}_{days}_{min_orders}"
        )
        
        # Executa busca com enhanced chunking
        result = _apply_enhanced_chunking(detector, days, min_orders, 25)
        
        # Update progress: 80%
        cache_manager.set(
            'async_job_status',
            {
                'status': 'running',
                'progress': 80,
                'message': 'Finalizando processamento e organizando resultados...',
                'updated_at': timezone.now().isoformat()
            },
            ttl=7200,
            job_id=f"job_{config_id}_{days}_{min_orders}"
        )
        
        # ===== FINALIZA√á√ÉO =====
        processing_time = time.time() - start_time
        
        # Salva resultado final no cache
        final_result = {
            'success': True,
            'data': result,
            'completed_at': timezone.now().isoformat(),
            'processing_time_seconds': round(processing_time, 2),
            'job_version': 'v2_async_enhanced'
        }
        
        cache_manager.set(
            'async_job_result',
            final_result,
            ttl=3600,  # 1 hora de cache
            job_id=f"job_{config_id}_{days}_{min_orders}"
        )
        
        # Update progress: 100% - Conclu√≠do
        cache_manager.set(
            'async_job_status',
            {
                'status': 'finished',
                'progress': 100,
                'message': f'Processamento conclu√≠do em {processing_time:.1f}s',
                'completed_at': timezone.now().isoformat(),
                'total_ips_found': result.get('total_ips_found', 0),
                'total_orders_analyzed': result.get('total_orders_analyzed', 0)
            },
            ttl=7200,
            job_id=f"job_{config_id}_{days}_{min_orders}"
        )
        
        # Log de sucesso no banco
        ProcessamentoLog.objects.create(
            user=user,
            config=config,
            tipo='busca_ip_async_v2',
            status='sucesso',
            pedidos_encontrados=result.get('total_orders_analyzed', 0),
            detalhes={
                'days': days,
                'min_orders': min_orders,
                'total_ips_found': result.get('total_ips_found', 0),
                'processing_method': 'async_job_v2',
                'processing_time_seconds': processing_time,
                'chunks_processed': result.get('chunks_processed', 0)
            }
        )
        
        logger.info(
            f"‚úÖ Job ass√≠ncrono V2 conclu√≠do com sucesso - "
            f"IPs: {result.get('total_ips_found', 0)}, "
            f"Tempo: {processing_time:.2f}s"
        )
        
        return final_result
        
    except Exception as e:
        logger.error(f"‚ùå Erro no job ass√≠ncrono V2: {str(e)}")
        
        # Salva erro no cache
        cache_manager = get_cache_manager()
        error_result = {
            'success': False,
            'error': str(e),
            'error_type': type(e).__name__,
            'failed_at': timezone.now().isoformat(),
            'job_version': 'v2_async_enhanced'
        }
        
        cache_manager.set(
            'async_job_result',
            error_result,
            ttl=1800,  # 30 minutos para erro
            job_id=f"job_{config_id}_{days}_{min_orders}"
        )
        
        # Update status: Error
        cache_manager.set(
            'async_job_status',
            {
                'status': 'failed',
                'progress': 0,
                'message': f'Processamento falhou: {str(e)}',
                'error': str(e),
                'failed_at': timezone.now().isoformat()
            },
            ttl=7200,
            job_id=f"job_{config_id}_{days}_{min_orders}"
        )
        
        # Log de erro no banco
        try:
            config = ShopifyConfig.objects.get(id=config_id)
            user = User.objects.get(id=user_id)
            ProcessamentoLog.objects.create(
                user=user,
                config=config,
                tipo='busca_ip_async_v2',
                status='erro',
                pedidos_encontrados=0,
                detalhes={
                    'days': days,
                    'min_orders': min_orders,
                    'error': str(e),
                    'error_type': type(e).__name__,
                    'processing_method': 'async_job_v2_failed'
                }
            )
        except Exception as log_error:
            logger.error(f"Erro ao salvar log de erro: {str(log_error)}")
        
        raise e

def _estimate_processing_time(days):
    """
    Estima tempo de processamento baseado no per√≠odo
    
    Args:
        days (int): N√∫mero de dias para processar
        
    Returns:
        int: Tempo estimado em minutos
    """
    if days <= 7:
        return 1  # 1 minuto
    elif days <= 30:
        return 3  # 3 minutos
    elif days <= 60:
        return 8  # 8 minutos
    elif days <= 120:
        return 15  # 15 minutos
    else:
        return 25  # 25 minutos m√°ximo

@csrf_exempt
@api_view(['GET'])
@permission_classes([IsAuthenticated])
def get_optimization_metrics(request):
    """
    Endpoint para monitorar m√©tricas das otimiza√ß√µes V2
    /api/processamento/optimization-metrics/
    """
    try:
        cache_manager = get_cache_manager()
        
        # Estat√≠sticas do cache
        cache_stats = cache_manager.get_stats()
        
        # Estat√≠sticas de rate limiting
        from .utils.security_utils import RateLimitManager
        rate_status = RateLimitManager.check_rate_limit(request.user, 'ip_search')
        
        # Jobs ass√≠ncronos recentes
        queue = get_queue('default')
        queue_info = {
            'pending_jobs': len(queue.get_jobs()),
            'failed_jobs': len(queue.failed_job_registry),
            'finished_jobs': len(queue.finished_job_registry)
        }
        
        optimization_metrics = {
            'cache_performance': cache_stats,
            'rate_limiting': {
                'allowed': rate_status[0],
                'remaining': rate_status[1]
            },
            'async_queue_status': queue_info,
            'optimization_version': 'v2_enhanced',
            'features_enabled': {
                'redis_cache': cache_manager.redis_available,
                'async_processing': True,
                'enhanced_chunking': True,
                'circuit_breaker': True,
                'progress_tracking': True
            },
            'performance_thresholds': {
                'sync_max_days': 30,
                'async_min_days': 31,
                'chunk_size_days': 15,
                'max_sync_timeout_seconds': 25
            }
        }
        
        return Response({
            'success': True,
            'metrics': optimization_metrics,
            'timestamp': timezone.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Erro ao obter m√©tricas de otimiza√ß√£o: {str(e)}")
        return Response({
            'error': 'Erro ao obter m√©tricas',
            'details': str(e)
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

# ===== FUN√á√ïES AUXILIARES PARA FALLBACK SEM REDIS =====

def _get_optimized_ip_data_no_redis(detector, days, min_orders):
    """
    Vers√£o otimizada que funciona sem Redis
    Usa apenas cache Django padr√£o e otimiza√ß√µes b√°sicas
    """
    import time
    from datetime import datetime
    
    try:
        logger.info(f"üîç Executando busca otimizada sem Redis - {days} dias")
        
        # Determina timeout baseado no per√≠odo
        if days <= 7:
            timeout = 15
        elif days <= 30:
            timeout = 25
        else:
            timeout = 35
        
        start_time = time.time()
        
        # Usa m√©todo b√°sico do detector com early break para otimiza√ß√£o
        result = detector.get_orders_by_ip(
            days=days, 
            min_orders=min_orders,
            early_break_threshold=100  # Para otimizar, para ap√≥s 100 IPs encontrados
        )
        
        processing_time = time.time() - start_time
        
        logger.info(f"‚úÖ Busca sem Redis conclu√≠da em {processing_time:.2f}s")
        
        # Adiciona metadata de otimiza√ß√£o
        if isinstance(result, dict):
            result['optimization_metadata'] = {
                'processing_time_seconds': round(processing_time, 2),
                'timeout_used': timeout,
                'redis_used': False,
                'optimization_level': 'basic_no_redis'
            }
        
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Erro na busca otimizada sem Redis: {str(e)}")
        # Fallback para m√©todo b√°sico
        return detector.get_orders_by_ip(days=min(days, 30), min_orders=min_orders)

def _get_basic_ip_data_with_timeout(detector, days, min_orders, timeout=45):
    """
    Vers√£o b√°sica com timeout estendido para per√≠odos grandes
    """
    import time
    from datetime import datetime
    
    try:
        logger.info(f"üìÖ Executando busca b√°sica com timeout {timeout}s - {days} dias")
        
        start_time = time.time()
        
        # Para per√≠odos grandes, divide em chunks de 30 dias
        if days > 30:
            logger.info(f"üì¶ Dividindo {days} dias em chunks para evitar timeout")
            
            all_results = []
            chunk_size = 30
            processed_days = 0
            
            while processed_days < days:
                chunk_days = min(chunk_size, days - processed_days)
                logger.info(f"üîç Processando chunk: dias {processed_days} a {processed_days + chunk_days}")
                
                try:
                    # Processa chunk com timeout menor
                    chunk_result = detector.get_orders_by_ip(
                        days=chunk_days,
                        min_orders=min_orders,
                        start_offset_days=processed_days
                    )
                    
                    if isinstance(chunk_result, dict) and 'ips_duplicados' in chunk_result:
                        all_results.extend(chunk_result['ips_duplicados'])
                    
                    processed_days += chunk_days
                    
                    # Pequena pausa entre chunks
                    time.sleep(1)
                    
                except Exception as chunk_error:
                    logger.warning(f"‚ùå Erro no chunk {processed_days}-{processed_days + chunk_days}: {chunk_error}")
                    # Continua com pr√≥ximo chunk
                    processed_days += chunk_days
            
            # Consolida resultados
            result = {
                'ips_duplicados': all_results,
                'total_ips_found': len(all_results),
                'chunked_processing': True,
                'total_chunks': (days + chunk_size - 1) // chunk_size
            }
        else:
            # Para per√≠odos menores, usa m√©todo normal
            result = detector.get_orders_by_ip(days=days, min_orders=min_orders)
        
        processing_time = time.time() - start_time
        
        logger.info(f"‚úÖ Busca b√°sica conclu√≠da em {processing_time:.2f}s")
        
        # Adiciona metadata
        if isinstance(result, dict):
            result['optimization_metadata'] = {
                'processing_time_seconds': round(processing_time, 2),
                'timeout_used': timeout,
                'method_used': 'chunked_basic' if days > 30 else 'basic'
            }
        
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Erro na busca b√°sica: {str(e)}")
        # √öltimo fallback - per√≠odo reduzido
        try:
            logger.warning(f"üîÑ Fallback para per√≠odo reduzido: 7 dias")
            return detector.get_orders_by_ip(days=7, min_orders=min_orders)
        except Exception as fallback_error:
            logger.error(f"‚ùå Fallback tamb√©m falhou: {fallback_error}")
            raise e


@api_view(['POST'])
@permission_classes([IsAuthenticated])
def buscar_ips_duplicados_simples(request):
    """
    Busca IPs com m√∫ltiplos pedidos - ALGORITMO CORRIGIDO DE 3 ETAPAS
    
    ETAPA 1: DESCOBERTA (com per√≠odo escolhido)
    - Busca pedidos nos √∫ltimos X dias (7/15/30/60/90 dias conforme selecionado)
    - Filtra apenas pedidos com note_attributes "IP address"
    - Identifica IPs com m√∫ltiplos clientes diferentes
    
    ETAPA 2: BUSCA COMPLETA POR IP (ANTES de gerar tabela)
    - Para cada IP candidato descoberto na Etapa 1:
    - Faz busca COMPLETA de TODOS os pedidos daquele IP espec√≠fico (sem limite temporal)
    - Usa a mesma l√≥gica que a fun√ß√£o de "Ver Detalhes"
    - Conta TODOS os pedidos hist√≥ricos do IP
    
    ETAPA 3: TABELA SINCRONIZADA
    - Mostra contagem real baseada na busca completa
    - Agora tabela e "Ver Detalhes" ter√£o os mesmos n√∫meros
    """
    try:
        loja_id = request.data.get('loja_id')
        days = request.data.get('days', 30)  # Padr√£o 30 dias
        
        # ‚ö° VALIDA√á√ÉO DE PERFORMANCE: Limite per√≠odo para compensar remo√ß√£o do limit=250
        if days > 90:
            return Response({
                'error': 'Per√≠odo m√°ximo permitido √© 90 dias para garantir performance',
                'details': 'Com a corre√ß√£o aplicada, per√≠odos longos podem impactar a performance'
            }, status=status.HTTP_400_BAD_REQUEST)
        
        # Log de auditoria
        logger.info(f"Busca de IPs duplicados - Usu√°rio: {request.user.username} (ID: {request.user.id}), Loja: {loja_id}, Per√≠odo: {days} dias")
        
        if not loja_id:
            return Response({'error': 'ID da loja √© obrigat√≥rio'}, status=status.HTTP_400_BAD_REQUEST)
        
        config = ShopifyConfig.objects.filter(id=loja_id, ativo=True).first()
        if not config:
            return Response({'error': 'Loja n√£o encontrada'}, status=status.HTTP_400_BAD_REQUEST)
        
        # Inicializa conex√£o Shopify
        import shopify
        shop_url = config.shop_url
        if not shop_url.startswith('https://'):
            shop_url = f"https://{shop_url}"
        
        session = shopify.Session(shop_url, "2024-07", config.access_token)
        shopify.ShopifyResource.activate_session(session)
        
        # ‚ö° ETAPA 1: DESCOBERTA DE IPs CANDIDATOS (com per√≠odo escolhido)
        data_inicial = timezone.now() - timedelta(days=days)
        
        # Busca pedidos do per√≠odo escolhido para descoberta de IPs candidatos
        orders = []
        page_info = None
        page = 1
        total_paginas_buscadas = 0
        
        logger.info(f"üîÑ ETAPA 1: Iniciando descoberta de IPs candidatos nos √∫ltimos {days} dias")
        
        while True:
            try:
                if page_info:
                    # P√°ginas subsequentes usam page_info
                    api_orders = shopify.Order.find(
                        limit=250,
                        page_info=page_info,
                        fields='id,order_number,created_at,cancelled_at,total_price,currency,financial_status,fulfillment_status,customer,line_items,tags,client_details,note_attributes'
                    )
                else:
                    # Primeira p√°gina usa filtros de data
                    api_orders = shopify.Order.find(
                        status='any',
                        created_at_min=data_inicial.isoformat(),
                        limit=250,
                        fields='id,order_number,created_at,cancelled_at,total_price,currency,financial_status,fulfillment_status,customer,line_items,tags,client_details,note_attributes'
                    )
                
                if not api_orders:
                    logger.info(f"üìÑ P√°gina {page} vazia - finalizando busca")
                    break
                
                # Adiciona pedidos da p√°gina atual
                orders.extend(api_orders)
                total_paginas_buscadas += 1
                
                logger.info(f"üìÑ P√°gina {page}: {len(api_orders)} pedidos encontrados (Total acumulado: {len(orders)})")
                
                # Verifica se h√° pr√≥xima p√°gina usando headers do Shopify
                try:
                    # Tenta obter page_info do √∫ltimo request
                    page_info = None
                    if hasattr(shopify.ShopifyResource, 'connection') and hasattr(shopify.ShopifyResource.connection, 'response'):
                        link_header = shopify.ShopifyResource.connection.response.headers.get('Link', '')
                        if link_header:
                            # Extrai page_info do header Link
                            import re
                            match = re.search(r'<[^>]*page_info=([^&>]+)[^>]*>;\s*rel="next"', link_header)
                            if match:
                                page_info = match.group(1)
                                logger.info(f"üîó Pr√≥xima p√°gina encontrada: page_info={page_info[:20]}...")
                            else:
                                logger.info(f"üèÅ √öltima p√°gina alcan√ßada (sem rel=next no header)")
                                break
                        else:
                            logger.info(f"üèÅ √öltima p√°gina alcan√ßada (sem header Link)")
                            break
                    else:
                        # Fallback: se n√£o conseguir ler headers, para aqui
                        logger.warning(f"‚ö†Ô∏è  N√£o foi poss√≠vel ler headers para pr√≥xima p√°gina")
                        break
                        
                except Exception as header_error:
                    logger.warning(f"‚ö†Ô∏è  Erro ao processar headers da p√°gina: {header_error}")
                    break
                
                page += 1
                
                # ‚ö° LIMITE DE SEGURAN√áA AUMENTADO (era 50, agora 200 p√°ginas)
                if page > 200:
                    logger.warning(f"‚ö†Ô∏è  Limite de seguran√ßa de 200 p√°ginas atingido - parando busca")
                    break
                    
            except Exception as e:
                logger.error(f"‚ùå Erro na p√°gina {page}: {str(e)}")
                if page == 1:
                    # Se primeira p√°gina falhar, propaga erro
                    return Response({
                        'error': f'Erro ao buscar pedidos: {str(e)}',
                        'suggestion': 'Tente reduzir o per√≠odo de busca ou contate o suporte'
                    }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
                else:
                    # Se p√°ginas posteriores falharem, para a busca mas usa dados coletados
                    logger.warning(f"‚ö†Ô∏è  Parando busca na p√°gina {page} devido a erro, mas usando {len(orders)} pedidos j√° coletados")
                    break
        
        logger.info(f"‚úÖ Busca paginada conclu√≠da: {len(orders)} pedidos em {total_paginas_buscadas} p√°ginas")
        
        def extract_ip_from_order(order_dict):
            """
            ‚ö° AJUSTE 1: SIMPLIFICA√á√ÉO - APENAS note_attributes "IP address"
            Remove todos os outros m√©todos de extra√ß√£o de IP para focar apenas na fonte confi√°vel
            """
            
            # √öNICO M√âTODO: note_attributes "IP address" (√öNICA FONTE CONFI√ÅVEL)
            note_attributes = order_dict.get("note_attributes", [])
            if note_attributes:
                # Busca exata por "IP address" (caso espec√≠fico das lojas)
                for attr in note_attributes:
                    if isinstance(attr, dict):
                        name = attr.get("name", "")
                        value = attr.get("value", "")
                        
                        if name == "IP address" and value and isinstance(value, str):
                            value = value.strip()
                            if value and value != 'None':
                                return value, 'note_attributes.IP_address', 0.98
            
            # Se n√£o tiver note_attributes "IP address", IGNORA o pedido
            return None, 'none', 0.0
        
        def should_exclude_order(order_dict):
            """
            ‚ö° AJUSTE 2: REMOVER TODOS OS FILTROS DE EXCLUS√ÉO
            Incluir TODOS os pedidos independente do status (cancelados, reembolsados, etc.)
            """
            
            # SEM FILTROS DE EXCLUS√ÉO - TODOS OS PEDIDOS S√ÉO INCLU√çDOS
            # N√£o exclui pedidos cancelados
            # N√£o exclui pedidos reembolsados  
            # N√£o exclui pedidos por qualquer status
            
            return False  # Nunca exclui nenhum pedido
        
        # Verifica√ß√£o se orders foi retornado corretamente
        if not orders:
            logger.warning(f"Nenhum pedido retornado da API Shopify para o per√≠odo de {days} dias")
            return Response({
                'ips_duplicados': [],
                'total_ips': 0,
                'total_pedidos': 0,
                'days_searched': days,
                'loja_nome': config.nome_loja,
                'warning': 'Nenhum pedido encontrado no per√≠odo especificado',
                'statistics': {
                    'total_processed': 0,
                    'excluded_count': 0,
                    'unique_ips_found': 0,
                    'methods_used': {},
                    'api_limit_used': 500
                }
            })
        
        # Agrupa pedidos por IP usando m√∫ltiplas fontes
        ip_groups = {}
        total_processed = 0
        excluded_count = 0
        methods_used = {}
        limit_usado = False  # Indica se foi usado algum limite de pagina√ß√£o
        
        logger.info(f"Processando {len(orders)} pedidos encontrados na API Shopify")
        
        for order in orders:
            total_processed += 1
            order_dict = order.to_dict()
            
            # Verifica se deve excluir o pedido
            if should_exclude_order(order_dict):
                excluded_count += 1
                continue
            
            # Extrai IP usando m√∫ltiplos m√©todos
            ip_found, method_used, confidence = extract_ip_from_order(order_dict)
            
            # Conta m√©todos usados para estat√≠sticas
            methods_used[method_used] = methods_used.get(method_used, 0) + 1
            
            if ip_found:
                if ip_found not in ip_groups:
                    ip_groups[ip_found] = []
                
                # Nome do cliente com tratamento de encoding
                customer_name = 'N/A'
                customer = order_dict.get('customer', {})
                if isinstance(customer, dict):
                    first_name = customer.get('first_name', '') or ''
                    last_name = customer.get('last_name', '') or ''
                    customer_name = f"{first_name} {last_name}".strip()
                    if not customer_name:
                        customer_name = 'N/A'
                
                # Extrai dados completos do cliente para compatibilidade com frontend
                customer_email = customer.get('email', '') if isinstance(customer, dict) else ''
                customer_phone = customer.get('phone', '') if isinstance(customer, dict) else ''
                
                # Tenta obter dados de endere√ßo (com fallback para evitar erro)
                shipping_city = ''
                shipping_state = ''
                try:
                    shipping_address = order_dict.get('shipping_address', {})
                    if isinstance(shipping_address, dict):
                        shipping_city = shipping_address.get('city', '')
                        shipping_state = shipping_address.get('province', '')
                except Exception:
                    pass  # Continua sem dados de endere√ßo se houver erro
                
                ip_groups[ip_found].append({
                    'order_id': str(order_dict.get('id', '')),
                    'order_number': order_dict.get('name', '') or order_dict.get('order_number', ''),
                    'customer_name': customer_name,
                    'customer_email': customer_email,
                    'customer_phone': customer_phone,
                    'total_price': str(order_dict.get('total_price', '0.00')),
                    'currency': order_dict.get('currency', 'BRL'),
                    'created_at': order_dict.get('created_at', ''),
                    'cancelled_at': order_dict.get('cancelled_at'),  # Campo crucial para verificar cancelamento
                    'financial_status': order_dict.get('financial_status', ''),
                    'shipping_city': shipping_city,
                    'shipping_state': shipping_state,
                    'method_used': method_used,
                    'confidence': confidence
                })
        
        
        # ‚ö° ETAPA 1 (continua√ß√£o): Identifica IPs candidatos com CLIENTES DIFERENTES
        ips_candidatos = []
        for ip, pedidos in ip_groups.items():
            if len(pedidos) >= 2:  # IP com 2+ pedidos
                # Conta clientes √∫nicos para an√°lise
                clientes_unicos = set()
                for pedido in pedidos:
                    cliente = pedido.get('customer_name', 'N/A')
                    if cliente and cliente != 'N/A':
                        clientes_unicos.add(cliente)
                
                # S√ì INCLUI se h√° CLIENTES DIFERENTES no mesmo IP
                if len(clientes_unicos) > 1:  # Mais de 1 cliente √∫nico = suspeito
                    ips_candidatos.append(ip)
                    logger.info(f"[ETAPA 1] IP candidato {ip}: {len(pedidos)} pedidos, {len(clientes_unicos)} clientes √∫nicos")
        
        logger.info(f"‚úÖ ETAPA 1 conclu√≠da: {len(ips_candidatos)} IPs candidatos encontrados")
        
        # ‚ö° ETAPA 2: BUSCA COMPLETA POR IP (SEM LIMITE TEMPORAL)
        # Para cada IP candidato, busca TODOS os pedidos hist√≥ricos daquele IP
        ips_duplicados = []
        
        logger.info(f"üîÑ ETAPA 2: Iniciando busca completa para cada IP candidato")
        
        for ip_candidato in ips_candidatos:
            try:
                logger.info(f"[ETAPA 2] Buscando TODOS os pedidos hist√≥ricos do IP {ip_candidato}")
                
                # Usa o mesmo m√©todo que "Ver Detalhes" para busca completa
                detector = ShopifyDuplicateOrderDetector(config.shop_url, config.access_token)
                
                # Busca TODOS os pedidos hist√≥ricos do IP (sem limite temporal)
                all_orders_for_ip = detector.get_orders_for_specific_ip(
                    target_ip=ip_candidato,
                    days=3650,  # ~10 anos = busca completa hist√≥rica
                    max_orders=500  # Limite alto para garantir que pega tudo
                )
                
                if not all_orders_for_ip:
                    logger.warning(f"[ETAPA 2] Nenhum pedido encontrado para IP {ip_candidato} na busca completa")
                    continue
                
                # Processa os dados para o formato esperado pelo frontend
                client_details = []
                clientes_unicos = set()
                
                for order in all_orders_for_ip:
                    # Extrai dados do cliente
                    customer = order.get('customer', {})
                    customer_name = f"{customer.get('first_name', '')} {customer.get('last_name', '')}".strip()
                    if not customer_name:
                        customer_name = 'Cliente n√£o informado'
                    
                    clientes_unicos.add(customer_name)
                    
                    # Determina status do pedido
                    is_cancelled = order.get('cancelled_at') is not None
                    
                    client_details.append({
                        'order_id': str(order['id']),
                        'order_number': order.get('order_number', ''),
                        'customer_name': customer_name,
                        'customer_email': customer.get('email', ''),
                        'customer_phone': customer.get('phone', ''),
                        'total_price': str(order.get('total_price', '0.00')),
                        'currency': order.get('currency', 'BRL'),
                        'created_at': order.get('created_at', ''),
                        'cancelled_at': order.get('cancelled_at'),
                        'financial_status': order.get('financial_status', ''),
                        'shipping_city': '',  # Ser√° preenchido se necess√°rio
                        'shipping_state': '',  # Ser√° preenchido se necess√°rio
                        'method_used': 'note_attributes.IP_address',
                        'confidence': 0.98
                    })
                
                # Ordena por data
                client_details_ordenados = sorted(client_details, key=lambda x: x['created_at'])
                
                # Adiciona √† lista final com contagem REAL de todos os pedidos
                ips_duplicados.append({
                    'browser_ip': ip_candidato,
                    'total_pedidos': len(all_orders_for_ip),  # CONTAGEM REAL de TODOS os pedidos
                    'clientes_unicos': len(clientes_unicos),
                    'clientes_diferentes': True,
                    'pedidos': client_details_ordenados,
                    'primeiro_pedido': client_details_ordenados[0]['created_at'] if client_details_ordenados else '',
                    'ultimo_pedido': client_details_ordenados[-1]['created_at'] if client_details_ordenados else '',
                    'method_used': 'note_attributes.IP_address',
                    'confidence': 0.98
                })
                
                logger.info(f"[ETAPA 2] IP {ip_candidato}: {len(all_orders_for_ip)} pedidos TOTAIS encontrados, {len(clientes_unicos)} clientes √∫nicos")
                
            except Exception as ip_error:
                logger.error(f"[ETAPA 2] Erro ao buscar pedidos completos do IP {ip_candidato}: {str(ip_error)}")
                continue
        
        logger.info(f"‚úÖ ETAPA 2 conclu√≠da: Busca completa realizada para {len(ips_duplicados)} IPs")
        
        # Ordena por quantidade de pedidos (mais pedidos primeiro)
        ips_duplicados.sort(key=lambda x: x['total_pedidos'], reverse=True)
        
        # Verifica√ß√£o final de consist√™ncia
        if total_processed > 0 and len(ip_groups) == 0:
            logger.error(f"ERRO: Processamos {total_processed} pedidos mas n√£o encontramos nenhum IP. Poss√≠vel problema na extra√ß√£o de IPs.")
        elif total_processed > 0 and len(ips_duplicados) == 0:
            logger.info(f"INFO: Processamos {total_processed} pedidos e encontramos {len(ip_groups)} IPs √∫nicos, mas nenhum com clientes diferentes.")
        
        # Log da busca melhorado com informa√ß√µes de debug
        total_pedidos_encontrados = sum(ip['total_pedidos'] for ip in ips_duplicados)
        
        logger.info(f"Busca conclu√≠da - IPs duplicados: {len(ips_duplicados)}, Pedidos processados: {total_processed}, Pedidos com IPs duplicados: {total_pedidos_encontrados}")
        
        create_safe_log(
            user=request.user,
            config=config,
            tipo='busca_ips_simples',
            status='sucesso',
            dados={
                'ips_encontrados': len(ips_duplicados),
                'days_searched': days,
                'total_processed': total_processed,
                'excluded_count': excluded_count,
                'unique_ips': len(ip_groups),
                'methods_used': methods_used,
                'version': 'ajustado_v4_simplificado',
                'pedidos_encontrados': total_pedidos_encontrados,
                'api_limit_used': limit_usado,
                'algoritmo_aplicado': '3_etapas_sincronizado',
                'etapas_executadas': [
                    'etapa1_descoberta_periodo',
                    'etapa2_busca_completa_historica',
                    'etapa3_tabela_sincronizada'
                ],
                'ips_candidatos_descobertos': len(ips_candidatos) if 'ips_candidatos' in locals() else 0
            }
        )
        
        return Response({
            'ips_duplicados': ips_duplicados,
            'total_ips': len(ips_duplicados),
            'total_pedidos': sum(ip['total_pedidos'] for ip in ips_duplicados),
            'days_searched': days,
            'loja_nome': config.nome_loja,
            'statistics': {
                'total_processed': total_processed,
                'excluded_count': excluded_count,
                'unique_ips_found': len(ip_groups),
                'methods_used': methods_used,
                'success_rate': len(ip_groups) / max(total_processed - excluded_count, 1) * 100,
                'api_limit_used': limit_usado
            }
        })
        
    except Exception as e:
        # Log do erro
        if 'config' in locals():
            create_safe_log(
                user=request.user,
                config=config,
                tipo='busca_ips_simples',
                status='erro',
                dados={'erro_mensagem': str(e)}
            )
        
        return Response({'error': str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

@csrf_exempt
@api_view(['POST'])
def debug_buscar_ip_especifico(request):
    """Debug espec√≠fico para encontrar um IP em TODOS os pedidos"""
    try:
        loja_id = request.data.get('loja_id')
        ip_procurado = request.data.get('ip')
        days = request.data.get('days', 365)  # 1 ano por padr√£o
        
        if not loja_id or not ip_procurado:
            return Response({'error': 'loja_id e ip s√£o obrigat√≥rios'}, status=status.HTTP_400_BAD_REQUEST)
        
        config = ShopifyConfig.objects.filter(id=loja_id, ativo=True).first()
        if not config:
            return Response({'error': 'Loja n√£o encontrada'}, status=status.HTTP_400_BAD_REQUEST)
        
        # Inicializa conex√£o Shopify
        import shopify
        shop_url = config.shop_url
        if not shop_url.startswith('https://'):
            shop_url = f"https://{shop_url}"
        
        session = shopify.Session(shop_url, "2024-07", config.access_token)
        shopify.ShopifyResource.activate_session(session)
        
        # ‚ö° CORRE√á√ÉO CR√çTICA: Implementa pagina√ß√£o completa para detalhar IP espec√≠fico
        data_inicial = timezone.now() - timedelta(days=days)
        
        # Busca TODOS os pedidos do per√≠odo usando pagina√ß√£o
        orders = []
        page_info = None
        page = 1
        
        logger.info(f"üîç Iniciando busca paginada para IP {ip_procurado} nos √∫ltimos {days} dias")
        
        while True:
            try:
                if page_info:
                    # P√°ginas subsequentes usam page_info
                    api_orders = shopify.Order.find(
                        limit=250,
                        page_info=page_info,
                        fields='id,order_number,created_at,cancelled_at,total_price,currency,financial_status,fulfillment_status,customer,line_items,tags,browser_ip,client_details,note_attributes'
                    )
                else:
                    # Primeira p√°gina usa filtros de data
                    api_orders = shopify.Order.find(
                        status='any',
                        created_at_min=data_inicial.isoformat(),
                        limit=250,
                        fields='id,order_number,created_at,cancelled_at,total_price,currency,financial_status,fulfillment_status,customer,line_items,tags,browser_ip,client_details,note_attributes'
                    )
                
                if not api_orders:
                    break
                
                orders.extend(api_orders)
                logger.info(f"üìÑ P√°gina {page}: {len(api_orders)} pedidos (Total: {len(orders)})")
                
                # Verifica pr√≥xima p√°gina
                try:
                    page_info = None
                    if hasattr(shopify.ShopifyResource, 'connection') and hasattr(shopify.ShopifyResource.connection, 'response'):
                        link_header = shopify.ShopifyResource.connection.response.headers.get('Link', '')
                        if link_header:
                            import re
                            match = re.search(r'<[^>]*page_info=([^&>]+)[^>]*>;\s*rel="next"', link_header)
                            if match:
                                page_info = match.group(1)
                            else:
                                break
                        else:
                            break
                    else:
                        break
                except Exception:
                    break
                
                page += 1
                if page > 50:  # Limite de seguran√ßa
                    break
                    
            except Exception as e:
                logger.error(f"‚ùå Erro na busca paginada p√°gina {page}: {e}")
                if page == 1:
                    return Response({
                        'error': f'Erro ao buscar pedidos: {str(e)}',
                        'suggestion': 'Tente reduzir o per√≠odo de busca'
                    }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
                else:
                    break  # Usa dados coletados at√© agora
        
        logger.info(f"‚úÖ Busca paginada conclu√≠da: {len(orders)} pedidos para an√°lise do IP {ip_procurado}")
        
        pedidos_encontrados = []
        total_analisados = 0
        
        for order in orders:
            total_analisados += 1
            order_dict = order.to_dict()
            
            # Busca o IP em TODOS os campos poss√≠veis
            ips_encontrados = []
            
            # note_attributes
            note_attributes = order_dict.get('note_attributes', [])
            if isinstance(note_attributes, list):
                for note in note_attributes:
                    if isinstance(note, dict) and note.get('name') == 'IP address':
                        ip_value = note.get('value')
                        if ip_value and str(ip_value).strip() == ip_procurado:
                            ips_encontrados.append(('note_attributes', ip_value))
            
            # browser_ip direto
            browser_ip = order_dict.get('browser_ip')
            if browser_ip and str(browser_ip).strip() == ip_procurado:
                ips_encontrados.append(('browser_ip', browser_ip))
            
            # client_details.browser_ip
            client_details = order_dict.get('client_details', {})
            if isinstance(client_details, dict):
                client_browser_ip = client_details.get('browser_ip')
                if client_browser_ip and str(client_browser_ip).strip() == ip_procurado:
                    ips_encontrados.append(('client_details.browser_ip', client_browser_ip))
            
            # Se encontrou o IP em qualquer campo
            if ips_encontrados:
                customer = order_dict.get('customer', {})
                pedidos_encontrados.append({
                    'id': order_dict.get('id'),
                    'number': order_dict.get('number'),
                    'created_at': order_dict.get('created_at'),
                    'financial_status': order_dict.get('financial_status'),
                    'fulfillment_status': order_dict.get('fulfillment_status'),
                    'cancelled_at': order_dict.get('cancelled_at'),
                    'customer_name': f"{customer.get('first_name', '')} {customer.get('last_name', '')}".strip(),
                    'customer_email': customer.get('email'),
                    'ips_encontrados': ips_encontrados,
                    'total_price': order_dict.get('total_price'),
                })
        
        # Log da busca debug
        create_safe_log(
            user=request.user,
            config=config,
            tipo='debug',
            status='sucesso',
            dados={
                'ip_procurado': ip_procurado,
                'pedidos_encontrados': len(pedidos_encontrados),
                'total_analisados': total_analisados,
                'days_searched': days
            }
        )
        
        return Response({
            'ip_procurado': ip_procurado,
            'pedidos_encontrados': pedidos_encontrados,
            'total_encontrados': len(pedidos_encontrados),
            'total_analisados': total_analisados,
            'days_searched': days,
            'loja_nome': config.nome_loja
        })
        
    except Exception as e:
        if 'config' in locals():
            create_safe_log(
                user=request.user,
                config=config,
                tipo='debug',
                status='erro',
                dados={'erro_mensagem': str(e), 'ip_procurado': ip_procurado if 'ip_procurado' in locals() else 'unknown'}
            )
        
        return Response({'error': str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


@api_view(['GET'])
@permission_classes([IsAuthenticated])
def debug_detector_ip_user_data(request):
    """Endpoint tempor√°rio para debug - verifica dados do usu√°rio e lojas"""
    try:
        # Dados do usu√°rio
        user_data = {
            'user_id': request.user.id,
            'username': request.user.username,
            'is_authenticated': request.user.is_authenticated,
            'is_staff': request.user.is_staff,
        }
        
        # Lojas do usu√°rio atual
        user_lojas = ShopifyConfig.objects.filter(user=request.user, ativo=True)
        user_lojas_data = [
            {
                'id': loja.id,
                'nome_loja': loja.nome_loja,
                'shop_url': loja.shop_url,
                'ativo': loja.ativo,
                'created_at': loja.created_at,
                'user_id': loja.user.id if loja.user else None
            }
            for loja in user_lojas
        ]
        
        # Todas as lojas (para debug)
        all_lojas = ShopifyConfig.objects.filter(ativo=True)
        all_lojas_data = [
            {
                'id': loja.id,
                'nome_loja': loja.nome_loja,
                'shop_url': loja.shop_url,
                'user_id': loja.user.id if loja.user else None,
                'user_username': loja.user.username if loja.user else None
            }
            for loja in all_lojas
        ]
        
        return Response({
            'success': True,
            'user_data': user_data,
            'user_lojas_count': len(user_lojas_data),
            'user_lojas': user_lojas_data,
            'all_lojas_count': len(all_lojas_data),
            'all_lojas': all_lojas_data,
            'debug_info': {
                'endpoint_url': '/api/processamento/debug-detector-ip-user-data/',
                'timestamp': timezone.now().isoformat(),
                'purpose': 'Diagnosticar erro 400 no detector de IP'
            }
        })
        
    except Exception as e:
        logger.error(f"Erro no debug endpoint: {str(e)}")
        return Response({
            'success': False,
            'error': str(e),
            'debug_info': 'Erro interno no endpoint de debug'
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)